{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/22 21:56:18 WARN Utils: Your hostname, Chaitanyasumas-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.139 instead (on interface en0)\n",
      "24/08/22 21:56:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/22 21:56:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark.sql\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark: SparkSession = (SparkSession.builder.appName(\"pacdb\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \".spark\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set font to Times New Roman\n",
    "LATEX = False\n",
    "if LATEX:\n",
    "    mpl.rcParams['text.usetex'] = True\n",
    "    mpl.rcParams[\"font.family\"] = \"serif\"\n",
    "    mpl.rcParams[\"font.serif\"] = \"Times\"\n",
    "else:\n",
    "    mpl.rcParams['text.usetex'] = False\n",
    "    mpl.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    mpl.rcParams[\"mathtext.fontset\"] = \"stix\"\n",
    "    \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "import matplotlib_inline.backend_inline  # type: ignore\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "mpl.rcParams['axes.titleweight'] = 'bold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set-up - Dataset, True Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/22 21:56:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|school|sex|age|address|famsize|Pstatus|Medu|Fedu|   Mjob|    Fjob|reason|guardian|traveltime|studytime|failures|schoolsup|famsup|paid|activities|nursery|higher|internet|romantic|famrel|freetime|goout|Dalc|Walc|health|absences| G1| G2| G3|\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|    GP|  F| 18|      U|    GT3|      A|   4|   4|at_home| teacher|course|  mother|         2|        2|       0|      yes|    no|  no|        no|    yes|   yes|      no|      no|     4|       3|    4|   1|   1|     3|       6|  5|  6|  6|\n",
      "|    GP|  F| 17|      U|    GT3|      T|   1|   1|at_home|   other|course|  father|         1|        2|       0|       no|   yes|  no|        no|     no|   yes|     yes|      no|     5|       3|    3|   1|   1|     3|       4|  5|  5|  6|\n",
      "|    GP|  F| 15|      U|    LE3|      T|   1|   1|at_home|   other| other|  mother|         1|        2|       3|      yes|    no| yes|        no|    yes|   yes|     yes|      no|     4|       3|    2|   2|   3|     3|      10|  7|  8| 10|\n",
      "|    GP|  F| 15|      U|    GT3|      T|   4|   2| health|services|  home|  mother|         1|        3|       0|       no|   yes| yes|       yes|    yes|   yes|     yes|     yes|     3|       2|    2|   1|   1|     5|       2| 15| 14| 15|\n",
      "|    GP|  F| 16|      U|    GT3|      T|   3|   3|  other|   other|  home|  father|         1|        2|       0|       no|   yes| yes|        no|    yes|   yes|      no|      no|     4|       3|    2|   1|   2|     5|       4|  6| 10| 10|\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|school|sex|age|address|famsize|Pstatus|Medu|Fedu|   Mjob|    Fjob|reason|guardian|traveltime|studytime|failures|schoolsup|famsup|paid|activities|nursery|higher|internet|romantic|famrel|freetime|goout|Dalc|Walc|health|absences| G1| G2| G3|\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|    GP|  F| 18|      U|    GT3|      A|   4|   4|at_home| teacher|course|  mother|         2|        2|       0|      yes|    no|  no|        no|    yes|   yes|      no|      no|     4|       3|    4|   1|   1|     3|       4|  0| 11| 11|\n",
      "|    GP|  F| 17|      U|    GT3|      T|   1|   1|at_home|   other|course|  father|         1|        2|       0|       no|   yes|  no|        no|     no|   yes|     yes|      no|     5|       3|    3|   1|   1|     3|       2|  9| 11| 11|\n",
      "|    GP|  F| 15|      U|    LE3|      T|   1|   1|at_home|   other| other|  mother|         1|        2|       0|      yes|    no|  no|        no|    yes|   yes|     yes|      no|     4|       3|    2|   2|   3|     3|       6| 12| 13| 12|\n",
      "|    GP|  F| 15|      U|    GT3|      T|   4|   2| health|services|  home|  mother|         1|        3|       0|       no|   yes|  no|       yes|    yes|   yes|     yes|     yes|     3|       2|    2|   1|   1|     5|       0| 14| 14| 14|\n",
      "|    GP|  F| 16|      U|    GT3|      T|   3|   3|  other|   other|  home|  father|         1|        2|       0|       no|   yes|  no|        no|    yes|   yes|      no|      no|     4|       3|    2|   1|   2|     5|       0| 11| 13| 13|\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "math_df: pyspark.sql.DataFrame = spark.read.csv(\"data/student_performance/student-mat.csv\", header=True, inferSchema=True, sep=\";\")\n",
    "math_df.show(5)\n",
    "\n",
    "portugese_df: pyspark.sql.DataFrame = spark.read.csv(\"data/student_performance/student-por.csv\", header=True, inferSchema=True, sep=\";\")\n",
    "portugese_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: \n",
    "\n",
    "Filter: for students with absences > 10\n",
    "Join: None\n",
    "Group By: Guardian\n",
    "Agg: Avg, Max absences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------------+\n",
      "|guardian|max_absences|      avg_absences|\n",
      "+--------+------------+------------------+\n",
      "|  father|          21|14.818181818181818|\n",
      "|  mother|          75|19.727272727272727|\n",
      "|   other|          40|20.181818181818183|\n",
      "+--------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, avg\n",
    "result = (\n",
    "    math_df.filter(F.col(\"absences\") > 10)\n",
    "           .groupBy(\"guardian\")\n",
    "           .agg(max(\"absences\").alias(\"max_absences\"), avg(\"absences\").alias(\"avg_absences\"))\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAC Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_list: List[float] = [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1., 2., 4.]\n",
    "sampling_rate: float = 0.5\n",
    "m: int = 10\n",
    "c: float = 1e-6\n",
    "mi: float = 1./4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "24/08/22 21:56:38 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "|guardian|avg(absences)|max(absences)|\n",
      "+--------+-------------+-------------+\n",
      "|  father|          0.0|          0.0|\n",
      "|  mother|          0.0|          0.0|\n",
      "|   other|          0.0|          0.0|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_produce_one_sampled_output() missing 1 required positional argument: 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chaitanyasuma/pac/pacdb/version_one.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/version_one.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df\u001b[39m.\u001b[39mgroupBy(F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mguardian\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39magg(F\u001b[39m.\u001b[39mavg(\u001b[39m\"\u001b[39m\u001b[39mabsences\u001b[39m\u001b[39m\"\u001b[39m), F\u001b[39m.\u001b[39mmax(\u001b[39m\"\u001b[39m\u001b[39mabsences\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/version_one.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m pac_worker \u001b[39m=\u001b[39m PACWorker(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/version_one.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                 filter_col\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mabsences\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/version_one.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                 filter_value\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m10\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/version_one.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                 query_function\u001b[39m=\u001b[39mquery\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/version_one.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/version_one.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m result \u001b[39m=\u001b[39m pac_worker\u001b[39m.\u001b[39;49mestimate_noise(math_df)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/version_one.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m pac_worker\u001b[39m.\u001b[39mrelease_pac_value()\n",
      "File \u001b[0;32m~/pac/pacdb/pac_db_re/pac_worker.py:198\u001b[0m, in \u001b[0;36mPACWorker.estimate_noise\u001b[0;34m(self, df, sampling_rate)\u001b[0m\n\u001b[1;32m    195\u001b[0m zeroes: np\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(output\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_updateDataFrame(zeroes, Y)\u001b[39m.\u001b[39mshow()\n\u001b[0;32m--> 198\u001b[0m noise: List[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_estimate_hybrid_noise(sample_once\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_produce_one_sampled_output)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m noise\n",
      "File \u001b[0;32m~/pac/pacdb/pac_db_re/pac_worker.py:136\u001b[0m, in \u001b[0;36mPACWorker._estimate_hybrid_noise\u001b[0;34m(self, sample_once, max_mi, eta)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_estimate_hybrid_noise\u001b[39m(\n\u001b[1;32m    129\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    130\u001b[0m         sample_once: Callable[[], np\u001b[39m.\u001b[39mndarray],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \n\u001b[1;32m    135\u001b[0m     \u001b[39m# Use the identity matrix for our projection matrix\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     dimensions \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(sample_once())\n\u001b[1;32m    137\u001b[0m     proj_matrix: np\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meye(dimensions)\n\u001b[1;32m    139\u001b[0m     \u001b[39m# always assuming hybrid anisotropic\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: _produce_one_sampled_output() missing 1 required positional argument: 'df'"
     ]
    }
   ],
   "source": [
    "# initialise the worker and set all/any 4 query parameters\n",
    "from pac_db_re import PACWorker\n",
    "from pac_db_re import AggregationType\n",
    "from pac_db_re import FilterTypeEnum\n",
    "\n",
    "filter_type = FilterTypeEnum.GREATER_THAN\n",
    "agg_type = AggregationType.AVG\n",
    "\n",
    "def query(df):\n",
    "    return df.groupBy(F.col(\"guardian\")).agg(F.avg(\"absences\"), F.max(\"absences\"))\n",
    "\n",
    "pac_worker = PACWorker(\n",
    "                filter_col='absences',\n",
    "                filter_value='10',\n",
    "                filter_type=filter_type,\n",
    "                group_by_col='guardian',\n",
    "                agg_type=agg_type,\n",
    "                agg_col='absences',\n",
    "                query_function=query\n",
    "            )\n",
    "\n",
    "result = pac_worker.estimate_noise(math_df)\n",
    "\n",
    "pac_worker.release_pac_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate sampling n times\n",
    "pac_worker.sample_df(sampling_rate)\n",
    "pac_worker.calculate_noise()\n",
    "pac_worker.release_pac_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
