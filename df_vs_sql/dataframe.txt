Dataframe
== Parsed Logical Plan ==
'Project [count(1) AS count#3544L]
+- Filter (respondent_name#65 = University of Wisconsin Credit Union)
   +- Join Inner, (lei#988 = lei_2020#69)
      :- SubqueryAlias spark_catalog.default.loans
      :  +- Relation spark_catalog.default.loans[activity_year#987,lei#988,derived_msa-md#989,state_code#990,county_code#991,census_tract#992,conforming_loan_limit#993,derived_loan_product_type#994,derived_dwelling_category#995,derived_ethnicity#996,derived_race#997,derived_sex#998,action_taken#999,purchaser_type#1000,preapproval#1001,loan_type#1002,loan_purpose#1003,lien_status#1004,reverse_mortgage#1005,open-end_line_of_credit#1006,business_or_commercial_purpose#1007,loan_amount#1008,loan_to_value_ratio#1009,interest_rate#1010,... 75 more fields] parquet
      +- SubqueryAlias spark_catalog.default.banks
         +- Relation spark_catalog.default.banks[respondent_name#65,arid_2017#66,lei_2018#67,lei_2019#68,lei_2020#69] parquet

== Analyzed Logical Plan ==
count: bigint
Aggregate [count(1) AS count#3544L]
+- Filter (respondent_name#65 = University of Wisconsin Credit Union)
   +- Join Inner, (lei#988 = lei_2020#69)
      :- SubqueryAlias spark_catalog.default.loans
      :  +- Relation spark_catalog.default.loans[activity_year#987,lei#988,derived_msa-md#989,state_code#990,county_code#991,census_tract#992,conforming_loan_limit#993,derived_loan_product_type#994,derived_dwelling_category#995,derived_ethnicity#996,derived_race#997,derived_sex#998,action_taken#999,purchaser_type#1000,preapproval#1001,loan_type#1002,loan_purpose#1003,lien_status#1004,reverse_mortgage#1005,open-end_line_of_credit#1006,business_or_commercial_purpose#1007,loan_amount#1008,loan_to_value_ratio#1009,interest_rate#1010,... 75 more fields] parquet
      +- SubqueryAlias spark_catalog.default.banks
         +- Relation spark_catalog.default.banks[respondent_name#65,arid_2017#66,lei_2018#67,lei_2019#68,lei_2020#69] parquet

== Optimized Logical Plan ==
Aggregate [count(1) AS count#3544L]
+- Project
   +- Join Inner, (lei#988 = lei_2020#69)
      :- Project [lei#988]
      :  +- Filter isnotnull(lei#988)
      :     +- Relation spark_catalog.default.loans[activity_year#987,lei#988,derived_msa-md#989,state_code#990,county_code#991,census_tract#992,conforming_loan_limit#993,derived_loan_product_type#994,derived_dwelling_category#995,derived_ethnicity#996,derived_race#997,derived_sex#998,action_taken#999,purchaser_type#1000,preapproval#1001,loan_type#1002,loan_purpose#1003,lien_status#1004,reverse_mortgage#1005,open-end_line_of_credit#1006,business_or_commercial_purpose#1007,loan_amount#1008,loan_to_value_ratio#1009,interest_rate#1010,... 75 more fields] parquet
      +- Project [lei_2020#69]
         +- Filter ((isnotnull(respondent_name#65) AND (respondent_name#65 = University of Wisconsin Credit Union)) AND isnotnull(lei_2020#69))
            +- Relation spark_catalog.default.banks[respondent_name#65,arid_2017#66,lei_2018#67,lei_2019#68,lei_2020#69] parquet

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[], functions=[count(1)], output=[count#3544L])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=1563]
      +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#3547L])
         +- Project
            +- BroadcastHashJoin [lei#988], [lei_2020#69], Inner, BuildRight, false
               :- Filter isnotnull(lei#988)
               :  +- FileScan parquet spark_catalog.default.loans[lei#988] Batched: true, Bucketed: false (bucket column(s) not read), DataFilters: [isnotnull(lei#988)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/michael/projects/dpdb/pacdb/spark-warehouse/loans], PartitionFilters: [], PushedFilters: [IsNotNull(lei)], ReadSchema: struct<lei:string>
               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=1558]
                  +- Project [lei_2020#69]
                     +- Filter ((isnotnull(respondent_name#65) AND (respondent_name#65 = University of Wisconsin Credit Union)) AND isnotnull(lei_2020#69))
                        +- FileScan parquet spark_catalog.default.banks[respondent_name#65,lei_2020#69] Batched: true, DataFilters: [isnotnull(respondent_name#65), (respondent_name#65 = University of Wisconsin Credit Union), isno..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/michael/projects/dpdb/pacdb/spark-warehouse/banks], PartitionFilters: [], PushedFilters: [IsNotNull(respondent_name), EqualTo(respondent_name,University of Wisconsin Credit Union), IsNot..., ReadSchema: struct<respondent_name:string,lei_2020:string>

