{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 00:14:10 WARN Utils: Your hostname, Chaitanyasumas-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.0.0.105 instead (on interface en0)\n",
      "24/08/09 00:14:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/09 00:14:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark.sql\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark: SparkSession = (SparkSession.builder.appName(\"pacdb\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \".spark\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set font to Times New Roman\n",
    "LATEX = False\n",
    "if LATEX:\n",
    "    mpl.rcParams['text.usetex'] = True\n",
    "    mpl.rcParams[\"font.family\"] = \"serif\"\n",
    "    mpl.rcParams[\"font.serif\"] = \"Times\"\n",
    "else:\n",
    "    mpl.rcParams['text.usetex'] = False\n",
    "    mpl.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    mpl.rcParams[\"mathtext.fontset\"] = \"stix\"\n",
    "    \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "import matplotlib_inline.backend_inline  # type: ignore\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "mpl.rcParams['axes.titleweight'] = 'bold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_df: pyspark.sql.DataFrame = spark.read.csv(\"./data/student_performance/student-mat.csv\", header=True, inferSchema=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 00:14:15 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|school|sex|age|address|famsize|Pstatus|Medu|Fedu|   Mjob|    Fjob|reason|guardian|traveltime|studytime|failures|schoolsup|famsup|paid|activities|nursery|higher|internet|romantic|famrel|freetime|goout|Dalc|Walc|health|absences| G1| G2| G3|\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|    GP|  F| 18|      U|    GT3|      A|   4|   4|at_home| teacher|course|  mother|         2|        2|       0|      yes|    no|  no|        no|    yes|   yes|      no|      no|     4|       3|    4|   1|   1|     3|       6|  5|  6|  6|\n",
      "|    GP|  F| 17|      U|    GT3|      T|   1|   1|at_home|   other|course|  father|         1|        2|       0|       no|   yes|  no|        no|     no|   yes|     yes|      no|     5|       3|    3|   1|   1|     3|       4|  5|  5|  6|\n",
      "|    GP|  F| 15|      U|    LE3|      T|   1|   1|at_home|   other| other|  mother|         1|        2|       3|      yes|    no| yes|        no|    yes|   yes|     yes|      no|     4|       3|    2|   2|   3|     3|      10|  7|  8| 10|\n",
      "|    GP|  F| 15|      U|    GT3|      T|   4|   2| health|services|  home|  mother|         1|        3|       0|       no|   yes| yes|       yes|    yes|   yes|     yes|     yes|     3|       2|    2|   1|   1|     5|       2| 15| 14| 15|\n",
      "|    GP|  F| 16|      U|    GT3|      T|   3|   3|  other|   other|  home|  father|         1|        2|       0|       no|   yes| yes|        no|    yes|   yes|      no|      no|     4|       3|    2|   1|   2|     5|       4|  6| 10| 10|\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "math_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pacdb import PACDataFrame, PACOptions, SamplerOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = math_df\n",
    "\n",
    "query_name: str = \"count\"\n",
    "budget_list: List[float] = [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1., 2., 4.]\n",
    "sample_size: int = 3\n",
    "sampling_rate: float = 0.5\n",
    "m: int = 10\n",
    "c: float = 1e-6\n",
    "mi: float = 1./4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [guardian], value: [school: string, sex: string ... 31 more fields], type: GroupBy]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_df.groupBy(\"guardian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+-------+-------+-------+----+----+--------+--------+----------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|school|sex|age|address|famsize|Pstatus|Medu|Fedu|    Mjob|    Fjob|    reason|guardian|traveltime|studytime|failures|schoolsup|famsup|paid|activities|nursery|higher|internet|romantic|famrel|freetime|goout|Dalc|Walc|health|absences| G1| G2| G3|\n",
      "+------+---+---+-------+-------+-------+----+----+--------+--------+----------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|    GP|  M| 17|      U|    GT3|      T|   3|   2|services|services|    course|  mother|         1|        1|       3|       no|   yes|  no|       yes|    yes|   yes|     yes|      no|     5|       5|    5|   2|   4|     5|      16|  6|  5|  5|\n",
      "|    GP|  F| 16|      U|    GT3|      T|   2|   2|services|services|      home|  mother|         1|        1|       2|       no|   yes| yes|        no|     no|   yes|     yes|      no|     1|       2|    2|   1|   3|     5|      14|  6|  9|  8|\n",
      "|    GP|  M| 16|      U|    GT3|      T|   4|   4| teacher| teacher|      home|  mother|         1|        2|       0|       no|   yes| yes|       yes|    yes|   yes|     yes|     yes|     4|       4|    5|   5|   5|     5|      16| 10| 12| 11|\n",
      "|    GP|  F| 16|      U|    LE3|      T|   2|   2|   other|   other|      home|  mother|         2|        2|       1|       no|   yes|  no|       yes|     no|   yes|     yes|     yes|     3|       3|    3|   1|   2|     3|      25|  7| 10| 11|\n",
      "|    GP|  F| 16|      U|    LE3|      T|   2|   2|   other| at_home|    course|  father|         2|        2|       1|      yes|    no|  no|       yes|    yes|   yes|     yes|      no|     4|       3|    3|   2|   2|     5|      14| 10| 10|  9|\n",
      "|    GP|  F| 16|      U|    LE3|      A|   3|   3|   other|services|      home|  mother|         1|        2|       0|       no|   yes|  no|        no|    yes|   yes|     yes|      no|     2|       3|    5|   1|   4|     3|      12| 11| 12| 11|\n",
      "|    GP|  F| 15|      R|    LE3|      T|   3|   1|   other|   other|reputation|  father|         2|        4|       0|       no|   yes|  no|        no|     no|   yes|     yes|      no|     4|       4|    2|   2|   3|     3|      12| 16| 16| 16|\n",
      "|    GP|  F| 16|      U|    GT3|      T|   3|   3|   other|services|      home|  mother|         1|        2|       0|      yes|   yes| yes|       yes|    yes|   yes|     yes|      no|     4|       3|    3|   2|   4|     5|      54| 11| 12| 11|\n",
      "|    GP|  F| 16|      U|    GT3|      T|   3|   4| at_home|   other|    course|  mother|         1|        2|       0|       no|   yes|  no|        no|    yes|   yes|     yes|      no|     2|       4|    3|   1|   2|     3|      12|  5|  5|  5|\n",
      "|    GP|  M| 16|      U|    GT3|      T|   2|   2|services|   other|reputation|  father|         2|        2|       1|       no|    no| yes|       yes|     no|   yes|     yes|      no|     4|       4|    2|   1|   1|     3|      12| 11| 10| 10|\n",
      "|    GP|  M| 16|      U|    LE3|      A|   4|   4| teacher|  health|reputation|  mother|         1|        2|       0|       no|   yes|  no|        no|    yes|   yes|      no|      no|     4|       1|    3|   3|   5|     5|      18|  8|  6|  7|\n",
      "|    GP|  M| 16|      U|    GT3|      T|   4|   4|services|services|     other|  mother|         1|        1|       0|      yes|   yes| yes|       yes|    yes|   yes|     yes|      no|     4|       5|    5|   5|   5|     4|      14|  7|  7|  5|\n",
      "|    GP|  F| 15|      U|    GT3|      T|   3|   2|services|   other|      home|  mother|         2|        2|       0|      yes|   yes| yes|        no|    yes|   yes|     yes|      no|     4|       3|    5|   1|   1|     2|      26|  7|  6|  6|\n",
      "|    GP|  M| 17|      R|    GT3|      T|   1|   3|   other|   other|    course|  father|         3|        2|       1|       no|   yes|  no|       yes|    yes|   yes|     yes|      no|     5|       2|    4|   1|   4|     5|      20|  9|  7|  8|\n",
      "|    GP|  M| 16|      U|    GT3|      T|   4|   4|  health|   other|    course|  mother|         1|        1|       0|       no|   yes|  no|       yes|    yes|   yes|     yes|      no|     3|       4|    4|   1|   4|     5|      18| 14| 11| 13|\n",
      "|    GP|  F| 17|      U|    LE3|      T|   2|   2|   other|   other|    course|  father|         1|        1|       0|       no|   yes|  no|        no|    yes|   yes|     yes|     yes|     3|       4|    4|   1|   3|     5|      12| 10| 13| 12|\n",
      "|    GP|  F| 16|      U|    GT3|      A|   3|   4|services|   other|    course|  father|         1|        1|       0|       no|    no|  no|        no|    yes|   yes|     yes|      no|     3|       2|    1|   1|   4|     5|      16| 12| 11| 11|\n",
      "|    GP|  M| 16|      U|    GT3|      T|   3|   2|services|services|    course|  mother|         2|        1|       1|       no|   yes|  no|       yes|     no|    no|      no|      no|     4|       5|    2|   1|   1|     2|      16| 12| 11| 12|\n",
      "|    GP|  F| 17|      U|    LE3|      T|   3|   3|   other|   other|reputation|  mother|         1|        2|       0|       no|   yes|  no|       yes|    yes|   yes|     yes|     yes|     5|       3|    3|   2|   3|     1|      56|  9|  9|  8|\n",
      "|    GP|  F| 16|      U|    GT3|      T|   3|   2|   other|   other|reputation|  mother|         1|        2|       0|       no|   yes| yes|        no|    yes|   yes|     yes|      no|     1|       2|    2|   1|   2|     1|      14| 12| 13| 12|\n",
      "+------+---+---+-------+-------+-------+----+----+--------+--------+----------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intm_math_df = math_df.filter(F.col(\"absences\") > 10)\n",
    "intm_math_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intm_math_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|guardian|count|\n",
      "+--------+-----+\n",
      "|  father|   90|\n",
      "|  mother|  273|\n",
      "|   other|   32|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/09 00:14:24 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "grouped_count_df = math_df.groupBy(\"guardian\").count()\n",
    "grouped_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Testing\n",
    "\n",
    "A. Without Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Aggregation - Avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|guardian|      avg_absences|\n",
      "+--------+------------------+\n",
      "|  father|14.818181818181818|\n",
      "|  mother|19.727272727272727|\n",
      "|   other|20.181818181818183|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, avg\n",
    "result = (\n",
    "    math_df.filter(F.col(\"absences\") > 10)\n",
    "           .groupBy(\"guardian\")\n",
    "           .agg(avg(\"absences\").alias(\"avg_absences\"))\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|guardian|     avg_absences|\n",
      "+--------+-----------------+\n",
      "|  father|3.977777777777778|\n",
      "|  mother|5.835164835164835|\n",
      "|   other|              9.5|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, avg\n",
    "result = (\n",
    "    math_df\n",
    "           .groupBy(\"guardian\")\n",
    "           .agg(avg(\"absences\").alias(\"avg_absences\"))\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAC Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found output format of query: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Noise: Using the identity matrix as the projection matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [17:20, ?it/s]\n",
      "0it [00:10, ?it/s]\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [3.63043478 6.12413793 5.47058824] + Noise = Noised: [3.427957486106539, 5.506936399576095, 6.8172503379030855]\n",
      "Inserting to dataframe:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|guardian|     avg(absences)|\n",
      "+--------+------------------+\n",
      "|  father| 3.427957486106539|\n",
      "|  mother| 5.506936399576095|\n",
      "|   other|6.8172503379030855|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query(df):\n",
    "    return df.groupBy(F.col(\"guardian\")).agg(F.avg(\"absences\"))\n",
    "\n",
    "mi = 1\n",
    "pac_df = (PACDataFrame.fromDataFrame(df)\n",
    "                    .withOptions(PACOptions(trials = m, max_mi = mi, c = c))\n",
    "                    .withSamplerOptions(SamplerOptions(fraction=sampling_rate))\n",
    "                    .withQuery(lambda x: query(x)))\n",
    "\n",
    "noise = pac_df.releaseValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Aggregation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----------------+\n",
      "|guardian|max_absences|     avg_absences|\n",
      "+--------+------------+-----------------+\n",
      "|  father|          21|3.977777777777778|\n",
      "|  mother|          75|5.835164835164835|\n",
      "|   other|          40|              9.5|\n",
      "+--------+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, avg\n",
    "result = math_df.groupBy(\"guardian\").agg(max(\"absences\").alias(\"max_absences\"), avg(\"absences\").alias(\"avg_absences\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAC Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found output format of query: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Noise: Using the identity matrix as the projection matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [01:16, ?it/s]\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [ 3.88888889  4.796875    7.47368421 21.         30.         20.        ] + Noise = Noised: [11.234769083145151, 11.314013701544233, -11.833881064202387, 81.89639744954465, 10.784551298126619, 165.59680231801315]\n",
      "Inserting to dataframe:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------------------+\n",
      "|guardian|      avg(absences)|     max(absences)|\n",
      "+--------+-------------------+------------------+\n",
      "|  father| 11.234769083145151| 81.89639744954465|\n",
      "|  mother| 11.314013701544233|10.784551298126619|\n",
      "|   other|-11.833881064202387|165.59680231801315|\n",
      "+--------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query(df):\n",
    "    return df.groupBy(F.col(\"guardian\")).agg(F.avg(\"absences\"), F.max(\"absences\"))\n",
    "\n",
    "mi = 1\n",
    "pac_df = (PACDataFrame.fromDataFrame(df)\n",
    "                    .withOptions(PACOptions(trials = m, max_mi = mi, c = c))\n",
    "                    .withSamplerOptions(SamplerOptions(fraction=sampling_rate))\n",
    "                    .withQuery(lambda x: query(x)))\n",
    "\n",
    "noise = pac_df.releaseValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. With Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Aggregation - Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Noise: Using the identity matrix as the projection matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:10, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[school: string, sex: string, age: int, address: string, famsize: string, Pstatus: string, Medu: int, Fedu: int, Mjob: string, Fjob: string, reason: string, guardian: string, traveltime: int, studytime: int, failures: int, schoolsup: string, famsup: string, paid: string, activities: string, nursery: string, higher: string, internet: string, romantic: string, famrel: int, freetime: int, goout: int, Dalc: int, Walc: int, health: int, absences: int, G1: int, G2: int, G3: int]\n",
      "guardian\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "applyGroupBy() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m pac_df\u001b[39m.\u001b[39msetGroupBy(\u001b[39m'\u001b[39m\u001b[39mguardian\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m pac_df\u001b[39m.\u001b[39msetAgg(\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m noise \u001b[39m=\u001b[39m pac_df\u001b[39m.\u001b[39;49mreleaseValue(threshold\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, threshold_value\u001b[39m=\u001b[39;49m\u001b[39m91\u001b[39;49m)\n",
      "File \u001b[0;32m~/pac/pacdb/pacdb/main.py:280\u001b[0m, in \u001b[0;36mPACDataFrame.releaseValue\u001b[0;34m(self, quiet, threshold, threshold_value)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mprint\u001b[39m(final_X)\n\u001b[1;32m    279\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupByCol)\n\u001b[0;32m--> 280\u001b[0m dict_of_counts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapplyGroupBy(final_X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroupByCol)\n\u001b[1;32m    282\u001b[0m new_dict \u001b[39m=\u001b[39m {key: value \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m dict_of_counts\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    284\u001b[0m final_Y: DataFrame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_applyQuery(final_X) \u001b[39m# DF: col 1: guardian col 2: avg\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: applyGroupBy() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "def query(df):\n",
    "    return df.groupBy(F.col(\"guardian\")).agg(F.avg(\"absences\"))\n",
    "\n",
    "mi = 1\n",
    "pac_df = (PACDataFrame.fromDataFrame(df)\n",
    "                    .withOptions(PACOptions(trials = m, max_mi = mi, c = c))\n",
    "                    .withSamplerOptions(SamplerOptions(fraction=sampling_rate))\n",
    "                    .withQuery(lambda x: query(x)))\n",
    "\n",
    "\n",
    "pac_df.setGroupBy('guardian')\n",
    "pac_df.setAgg('avg')\n",
    "\n",
    "noise = pac_df.releaseValue(threshold=True, threshold_value=91)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(df):\n",
    "    return df.groupBy(F.col(\"guardian\")).agg(F.avg(\"absences\"), F.max(\"absences\"))\n",
    "\n",
    "mi = 1\n",
    "pac_df = (PACDataFrame.fromDataFrame(df)\n",
    "                    .withOptions(PACOptions(trials = m, max_mi = mi, c = c))\n",
    "                    .withSamplerOptions(SamplerOptions(fraction=sampling_rate))\n",
    "                    .withQuery(lambda x: query(x)))\n",
    "\n",
    "noise = pac_df.releaseValue(threshold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/11 23:46:17 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found output format of query: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-------------+\n",
      "|guardian|    avg(absences)|max(absences)|\n",
      "+--------+-----------------+-------------+\n",
      "|  father|4.386363636363637|           21|\n",
      "|  mother|6.977777777777778|           75|\n",
      "|   other|7.733333333333333|           40|\n",
      "+--------+-----------------+-------------+\n",
      "\n",
      "chai_debug: The identity matrix dimensions are : 2\n",
      "max_mi: 0.015625, eta: 0.5, dimensions: 2\n",
      "Hybrid Noise: Using the identity matrix as the projection matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m mi \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m64\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m32\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m16\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m8\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m4\u001b[39m, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m1.\u001b[39m, \u001b[39m2.\u001b[39m, \u001b[39m4.\u001b[39m]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     pac_df \u001b[39m=\u001b[39m (PACDataFrame\u001b[39m.\u001b[39mfromDataFrame(df)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                         \u001b[39m.\u001b[39mwithOptions(PACOptions(trials \u001b[39m=\u001b[39m m, max_mi \u001b[39m=\u001b[39m mi, c \u001b[39m=\u001b[39m c))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                         \u001b[39m.\u001b[39mwithSamplerOptions(SamplerOptions(fraction\u001b[39m=\u001b[39msampling_rate))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                         \u001b[39m.\u001b[39mwithQuery(\u001b[39mlambda\u001b[39;00m x: query(x)))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     noise \u001b[39m=\u001b[39m pac_df\u001b[39m.\u001b[39;49mreleaseValue()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chaitanyasuma/pac/pacdb/hybrid_test.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor MI = \u001b[39m\u001b[39m{\u001b[39;00mmi\u001b[39m}\u001b[39;00m\u001b[39m, Noise is: \u001b[39m\u001b[39m{\u001b[39;00mnoise\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/pac/pacdb/pacdb/main.py:279\u001b[0m, in \u001b[0;36mPACDataFrame.releaseValue\u001b[0;34m(self, quiet)\u001b[0m\n\u001b[1;32m    276\u001b[0m Y\u001b[39m.\u001b[39mshow()\n\u001b[1;32m    278\u001b[0m \u001b[39m# TODO: just send output shape to _estimate_hybrid_noise\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m noise: List[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_estimate_hybrid_noise(sample_output\u001b[39m=\u001b[39;49moutput, quiet\u001b[39m=\u001b[39;49mquiet)\n\u001b[1;32m    281\u001b[0m noised_output: np\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_noise(output, noise, quiet\u001b[39m=\u001b[39mquiet)\n\u001b[1;32m    283\u001b[0m output_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_updateDataFrame(noised_output, Y)\n",
      "File \u001b[0;32m~/pac/pacdb/pacdb/main.py:205\u001b[0m, in \u001b[0;36mPACDataFrame._estimate_hybrid_noise\u001b[0;34m(self, sample_output, max_mi, quiet)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m converged:\n\u001b[1;32m    200\u001b[0m     \u001b[39m# Step 1: sample\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[39m# Step 2: get output\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[39m# Step 3: update estimate lists\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39m# Step 4: Every 10 trials, Check for convergence\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     output, threshold_check_failed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_produce_one_sampled_output(sample_output)\n\u001b[0;32m--> 205\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(output) \u001b[39m==\u001b[39m dimensions\n\u001b[1;32m    207\u001b[0m     \u001b[39m# if not threshold_check_failed\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[39m# Full Anisotropic\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m quiet:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for mi in [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1., 2., 4.]:\n",
    "    pac_df = (PACDataFrame.fromDataFrame(df)\n",
    "                        .withOptions(PACOptions(trials = m, max_mi = mi, c = c))\n",
    "                        .withSamplerOptions(SamplerOptions(fraction=sampling_rate))\n",
    "                        .withQuery(lambda x: query(x)))\n",
    "\n",
    "    noise = pac_df.releaseValue()\n",
    "    print(f\"For MI = {mi}, Noise is: {noise}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(df):\n",
    "    return df.filter(df[\"absences\"] >= 5).agg(F.count(\"*\"))\n",
    "\n",
    "pac_df = (PACDataFrame.fromDataFrame(df)\n",
    "                    .withOptions(PACOptions(trials = m, max_mi = mi, c = c))\n",
    "                    .withSamplerOptions(SamplerOptions(fraction=sampling_rate))\n",
    "                    .withQuery(lambda x: query(x)))\n",
    "\n",
    "pac_df.releaseValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(df):\n",
    "    return df.groupBy(F.col(\"guardian\")).agg(F.count(\"*\"))\n",
    "\n",
    "pac_df = (PACDataFrame.fromDataFrame(df)\n",
    "                    .withOptions(PACOptions(trials = m, max_mi = mi, c = c))\n",
    "                    .withSamplerOptions(SamplerOptions(fraction=sampling_rate))\n",
    "                    .withQuery(lambda x: query(x)))\n",
    "\n",
    "pac_df.releaseValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Anisotropic Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_noise = []\n",
    "for mi in [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1., 2., 4.]:\n",
    "    pac_df = (PACDataFrame.fromDataFrame(df)\n",
    "                        .withOptions(PACOptions(trials = m, max_mi = mi, c = c))\n",
    "                        .withSamplerOptions(SamplerOptions(fraction=sampling_rate))\n",
    "                        .withQuery(lambda x: query(x)))\n",
    "\n",
    "    noise = pac_df.releaseValue()\n",
    "    all_noise.append(noise)\n",
    "    print(f\"For MI = {mi}, Noise is: {noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_noise = []\n",
    "for mi in [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1., 2., 4.]:\n",
    "    pac_df = (PACDataFrame.fromDataFrame(df)\n",
    "                        .withOptions(PACOptions(trials = m, max_mi = mi, c = c))\n",
    "                        .withSamplerOptions(SamplerOptions(fraction=sampling_rate))\n",
    "                        .withQuery(lambda x: query(x)))\n",
    "\n",
    "    noise = pac_df.releaseValue()\n",
    "    all_noise.append(noise)\n",
    "    print(f\"For MI = {mi}, Noise is: {noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_noise_3 = []\n",
    "for mi in [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1., 2., 4.]:\n",
    "    pac_df = (PACDataFrame.fromDataFrame(df)\n",
    "                        .withOptions(PACOptions(trials = m, max_mi = mi, c = c))\n",
    "                        .withSamplerOptions(SamplerOptions(fraction=sampling_rate))\n",
    "                        .withQuery(lambda x: query(x)))\n",
    "\n",
    "    noise = pac_df.releaseValue()\n",
    "    all_noise_3.append(noise)\n",
    "    print(f\"For MI = {mi}, Noise is: {noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_values = [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4]\n",
    "\n",
    "etas = [0.025, 0.05, 0.1]\n",
    "\n",
    "# Define categories\n",
    "categories = {\n",
    "    'Category 1': 'Avg Father',\n",
    "    'Category 2': 'Avg Mother',\n",
    "    'Category 3': 'Avg Other',\n",
    "    'Category 4': 'Max Father',\n",
    "    'Category 5': 'Max Mother',\n",
    "    'Category 6': 'Max Other'\n",
    "}\n",
    "\n",
    "# Define colors for each category and each eta\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, (category, label) in enumerate(categories.items()):\n",
    "    for j, eta in enumerate(etas):\n",
    "        color = colors[j]  # Select color for current eta\n",
    "        plt.plot(x_values, y_values[category][j], label=f'eta={eta}', color=color)\n",
    "    \n",
    "    # Add category annotation above the first line in the category\n",
    "    plt.text(x_values[0], y_values[category][0][0], label, fontsize=10, color=colors[i*len(etas)], weight='bold', va='bottom', ha='left')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('X values (log scale)')\n",
    "plt.ylabel('Y values')\n",
    "plt.title('Y values for different categories and eta values')\n",
    "plt.xscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_for_max_mi(y_values, fig_number, x_values = [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1., 2., 4.]):\n",
    "    colors = ['blue', 'orange', 'green', 'red', 'purple', 'brown']\n",
    "    color_labels = ['Avg - Father', 'Avg - Mother', 'Avg - Other', 'Max - Father', 'Max - Mother', 'Max - Other']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for i in range(y_values.shape[1]):\n",
    "        ax.plot(x_values, y_values[:, i], label=color_labels[i])\n",
    "        \n",
    "    ax.set_xlabel('Mutual Information (Log Scale)')\n",
    "    ax.set_ylabel('Noise Estimate')\n",
    "    ax.set_title('Hybrid Anisotropic')\n",
    "    ax.set_xscale('log', base=2)\n",
    "    ax.set_xticks(x_values)\n",
    "    ax.set_xticklabels([f'MI = 1/{int(1/x_val)}' if x_val < 1 else f'MI = {x_val}' for x_val in x_values])\n",
    "    ax.legend()\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"figs/test_hybrid_{fig_number}.png\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values = np.array([[y_dict[i] for i in range(6)] for y_dict in all_noise])\n",
    "plot_for_max_mi(y_values, fig_number=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Anisotropic with Latest Changes by Mayuri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(df):\n",
    "    return df.groupBy(F.col(\"guardian\")).agg(F.avg(\"absences\"), F.max(\"absences\"))\n",
    "all_noise_new = []\n",
    "for mi in [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1., 2., 4.]:\n",
    "    pac_df = (PACDataFrame.fromDataFrame(df)\n",
    "                        .withOptions(PACOptions(trials = m, max_mi = mi, c = c))\n",
    "                        .withSamplerOptions(SamplerOptions(fraction=sampling_rate))\n",
    "                        .withQuery(lambda x: query(x)))\n",
    "\n",
    "    noise = pac_df.releaseValue()\n",
    "    all_noise_new.append(noise)\n",
    "    print(f\"For MI = {mi}, Noise is: {noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values_new = np.array([[y_dict[i] for i in range(6)] for y_dict in all_noise_new])\n",
    "print(y_values_new)\n",
    "plot_for_max_mi(y_values_new, fig_number=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Anisotropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion between dataframes and numpy vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-----------------+\n",
      "|guardian|max_absences|     avg_absences|\n",
      "+--------+------------+-----------------+\n",
      "|  father|          21|3.977777777777778|\n",
      "|  mother|          75|5.835164835164835|\n",
      "|   other|          40|              9.5|\n",
      "+--------+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, avg\n",
    "result = math_df.groupBy(\"guardian\").agg(max(\"absences\").alias(\"max_absences\"), avg(\"absences\").alias(\"avg_absences\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151]\n",
      "[ 90 273  32]\n",
      "[ 3.97777778  5.83516484  9.5        21.         75.         40.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(series.dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count(1)\n",
      "0       151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(series.dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count(1)\n",
      "0        90\n",
      "1       273\n",
      "2        32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(series.dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   avg(absences)  max(absences)\n",
      "0       3.977778           21.0\n",
      "1       5.835165           75.0\n",
      "2       9.500000           40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     151|\n",
      "+--------+\n",
      "\n",
      "+--------+--------+\n",
      "|guardian|count(1)|\n",
      "+--------+--------+\n",
      "|  father|      90|\n",
      "|  mother|     273|\n",
      "|   other|      32|\n",
      "+--------+--------+\n",
      "\n",
      "+--------+-----------------+-------------+\n",
      "|guardian|    avg(absences)|max(absences)|\n",
      "+--------+-----------------+-------------+\n",
      "|  father|3.977777777777778|         21.0|\n",
      "|  mother|5.835164835164835|         75.0|\n",
      "|   other|              9.5|         40.0|\n",
      "+--------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "import pyspark.sql.dataframe\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "df = math_df\n",
    "\n",
    "def _unwrapDataFrame(df: pyspark.sql.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a PySpark DataFrame into a numpy vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    numeric_columns: List[str] = [f.name for f in df.schema.fields if isinstance(f.dataType, T.NumericType)]\n",
    "    df_numeric: pyspark.sql.DataFrame = df.select(*numeric_columns)  # select only numeric columns\n",
    "    np_array: np.ndarray = np.array(df_numeric.collect())\n",
    "\n",
    "    flat: np.ndarray = np_array.flatten(order=\"F\")\n",
    "\n",
    "    return flat\n",
    "\n",
    "def _updateDataFrame(vec: np.ndarray, df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Use the values of the numpy vector to update the PySpark DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    numeric_columns: List[str] = [f.name for f in df.schema.fields if isinstance(f.dataType, T.NumericType)]\n",
    "    df_numeric: pyspark.sql.DataFrame = df.select(*numeric_columns)  # select only numeric columns\n",
    "    shape = np.array(df_numeric.collect()).shape\n",
    "\n",
    "    np_array = vec.reshape(shape, order=\"F\")\n",
    "    new_pandas: ps.DataFrame = ps.DataFrame(np_array, columns=numeric_columns)\n",
    "    print(new_pandas)\n",
    "\n",
    "    old_pandas = df.pandas_api()\n",
    "    old_pandas.update(new_pandas)\n",
    "\n",
    "    return old_pandas.to_spark()\n",
    "\n",
    "\n",
    "u1 = _unwrapDataFrame(df.filter(df[\"absences\"] >= 5).agg(F.count(\"*\")))\n",
    "u2 = _unwrapDataFrame(df.groupBy(F.col(\"guardian\")).agg(F.count(\"*\")))\n",
    "u3 = _unwrapDataFrame(df.groupBy(F.col(\"guardian\")).agg(F.avg(\"absences\"), F.max(\"absences\")))\n",
    "\n",
    "print(u1)\n",
    "print(u2)\n",
    "print(u3)\n",
    "\n",
    "r1 = _updateDataFrame(u1, df.filter(df[\"absences\"] >= 5).agg(F.count(\"*\")))\n",
    "r2 = _updateDataFrame(u2, df.groupBy(F.col(\"guardian\")).agg(F.count(\"*\")))\n",
    "r3 = _updateDataFrame(u3, df.groupBy(F.col(\"guardian\")).agg(F.avg(\"absences\"), F.max(\"absences\")))\n",
    "\n",
    "r1.show()\n",
    "r2.show()\n",
    "r3.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
