{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/23 16:04:52 WARN Utils: Your hostname, Chaitanyasumas-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.139 instead (on interface en0)\n",
      "24/08/23 16:04:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/23 16:04:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark.sql\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark: SparkSession = (SparkSession.builder.appName(\"pacdb\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \".spark\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set font to Times New Roman\n",
    "LATEX = False\n",
    "if LATEX:\n",
    "    mpl.rcParams['text.usetex'] = True\n",
    "    mpl.rcParams[\"font.family\"] = \"serif\"\n",
    "    mpl.rcParams[\"font.serif\"] = \"Times\"\n",
    "else:\n",
    "    mpl.rcParams['text.usetex'] = False\n",
    "    mpl.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    mpl.rcParams[\"mathtext.fontset\"] = \"stix\"\n",
    "    \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "import matplotlib_inline.backend_inline  # type: ignore\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "mpl.rcParams['axes.titleweight'] = 'bold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set-up - Dataset, True Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/23 16:04:57 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|school|sex|age|address|famsize|Pstatus|Medu|Fedu|   Mjob|    Fjob|reason|guardian|traveltime|studytime|failures|schoolsup|famsup|paid|activities|nursery|higher|internet|romantic|famrel|freetime|goout|Dalc|Walc|health|absences| G1| G2| G3|\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|    GP|  F| 18|      U|    GT3|      A|   4|   4|at_home| teacher|course|  mother|         2|        2|       0|      yes|    no|  no|        no|    yes|   yes|      no|      no|     4|       3|    4|   1|   1|     3|       6|  5|  6|  6|\n",
      "|    GP|  F| 17|      U|    GT3|      T|   1|   1|at_home|   other|course|  father|         1|        2|       0|       no|   yes|  no|        no|     no|   yes|     yes|      no|     5|       3|    3|   1|   1|     3|       4|  5|  5|  6|\n",
      "|    GP|  F| 15|      U|    LE3|      T|   1|   1|at_home|   other| other|  mother|         1|        2|       3|      yes|    no| yes|        no|    yes|   yes|     yes|      no|     4|       3|    2|   2|   3|     3|      10|  7|  8| 10|\n",
      "|    GP|  F| 15|      U|    GT3|      T|   4|   2| health|services|  home|  mother|         1|        3|       0|       no|   yes| yes|       yes|    yes|   yes|     yes|     yes|     3|       2|    2|   1|   1|     5|       2| 15| 14| 15|\n",
      "|    GP|  F| 16|      U|    GT3|      T|   3|   3|  other|   other|  home|  father|         1|        2|       0|       no|   yes| yes|        no|    yes|   yes|      no|      no|     4|       3|    2|   1|   2|     5|       4|  6| 10| 10|\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|school|sex|age|address|famsize|Pstatus|Medu|Fedu|   Mjob|    Fjob|reason|guardian|traveltime|studytime|failures|schoolsup|famsup|paid|activities|nursery|higher|internet|romantic|famrel|freetime|goout|Dalc|Walc|health|absences| G1| G2| G3|\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "|    GP|  F| 18|      U|    GT3|      A|   4|   4|at_home| teacher|course|  mother|         2|        2|       0|      yes|    no|  no|        no|    yes|   yes|      no|      no|     4|       3|    4|   1|   1|     3|       4|  0| 11| 11|\n",
      "|    GP|  F| 17|      U|    GT3|      T|   1|   1|at_home|   other|course|  father|         1|        2|       0|       no|   yes|  no|        no|     no|   yes|     yes|      no|     5|       3|    3|   1|   1|     3|       2|  9| 11| 11|\n",
      "|    GP|  F| 15|      U|    LE3|      T|   1|   1|at_home|   other| other|  mother|         1|        2|       0|      yes|    no|  no|        no|    yes|   yes|     yes|      no|     4|       3|    2|   2|   3|     3|       6| 12| 13| 12|\n",
      "|    GP|  F| 15|      U|    GT3|      T|   4|   2| health|services|  home|  mother|         1|        3|       0|       no|   yes|  no|       yes|    yes|   yes|     yes|     yes|     3|       2|    2|   1|   1|     5|       0| 14| 14| 14|\n",
      "|    GP|  F| 16|      U|    GT3|      T|   3|   3|  other|   other|  home|  father|         1|        2|       0|       no|   yes|  no|        no|    yes|   yes|      no|      no|     4|       3|    2|   1|   2|     5|       0| 11| 13| 13|\n",
      "+------+---+---+-------+-------+-------+----+----+-------+--------+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "math_df: pyspark.sql.DataFrame = spark.read.csv(\"data/student_performance/student-mat.csv\", header=True, inferSchema=True, sep=\";\")\n",
    "math_df.show(5)\n",
    "\n",
    "portugese_df: pyspark.sql.DataFrame = spark.read.csv(\"data/student_performance/student-por.csv\", header=True, inferSchema=True, sep=\";\")\n",
    "portugese_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: \n",
    "\n",
    "Filter: for students with absences > 10\n",
    "Join: None\n",
    "Group By: Guardian\n",
    "Agg: Avg, Max absences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------------+\n",
      "|guardian|max_absences|      avg_absences|\n",
      "+--------+------------+------------------+\n",
      "|  father|          21|              21.0|\n",
      "|  mother|          75|              35.0|\n",
      "|   other|          40|33.333333333333336|\n",
      "+--------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, avg\n",
    "result = (\n",
    "    math_df.filter(F.col(\"absences\") > 20)\n",
    "           .groupBy(\"guardian\")\n",
    "           .agg(max(\"absences\").alias(\"max_absences\"), avg(\"absences\").alias(\"avg_absences\"))\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAC Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_list: List[float] = [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1., 2., 4.]\n",
    "sampling_rate: float = 0.5\n",
    "m: int = 10\n",
    "c: float = 1e-6\n",
    "mi: float = 1./4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# initialise the worker and set all/any 4 query parameters\n",
    "from pac_db_re import PACWorker\n",
    "from pac_db_re import AggregationType\n",
    "from pac_db_re import FilterTypeEnum\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "filter_type = FilterTypeEnum.GREATER_THAN\n",
    "\n",
    "def query(df):\n",
    "    return (df\n",
    "            .groupBy(F.col(\"guardian\"))\n",
    "            .agg(F.avg(\"absences\"), F.max(\"absences\")))\n",
    "\n",
    "pac_worker = PACWorker(\n",
    "                filter_col='absences',\n",
    "                filter_value='20',\n",
    "                filter_type=filter_type,\n",
    "                group_by_col='guardian',\n",
    "                query_function=query\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "|guardian|avg(absences)|max(absences)|\n",
      "+--------+-------------+-------------+\n",
      "|  father|          0.0|          0.0|\n",
      "|  mother|          0.0|          0.0|\n",
      "|   other|          0.0|          0.0|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:3787: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "24/08/23 16:05:07 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [0. 6. 2.] + Noise = Noised: [-166.06335111349205, -5.336288302651644, 82.84371124388684]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "24/08/23 16:16:55 WARN AttachDistributedSequenceExec: clean up cached RDD(78861) in AttachDistributedSequenceExec(847067)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|guardian|              count|\n",
      "+--------+-------------------+\n",
      "|  father|-166.06335111349205|\n",
      "|  mother| -5.336288302651644|\n",
      "|   other|  82.84371124388684|\n",
      "+--------+-------------------+\n",
      "\n",
      "Sample: [ 0.          0.         34.83333333 56.         30.         38.        ] + Noise = Noised: [53.96623604976048, -247.60871752304345, 59.99983813691367, 393.61858155490916, -72.2916119334684, 151.97055257122628]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-------------------+\n",
      "|guardian|    avg(absences)|      max(absences)|\n",
      "+--------+-----------------+-------------------+\n",
      "|  father|53.96623604976048|-247.60871752304345|\n",
      "|  mother|59.99983813691367| 393.61858155490916|\n",
      "|   other|-72.2916119334684| 151.97055257122628|\n",
      "+--------+-----------------+-------------------+\n",
      "\n",
      "+--------+-----------------+-------------------+\n",
      "|guardian|    avg(absences)|      max(absences)|\n",
      "+--------+-----------------+-------------------+\n",
      "|  father|53.96623604976048|-247.60871752304345|\n",
      "|  mother|              0.0|                0.0|\n",
      "|   other|-72.2916119334684| 151.97055257122628|\n",
      "+--------+-----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/23 16:16:56 WARN AttachDistributedSequenceExec: clean up cached RDD(78953) in AttachDistributedSequenceExec(848035)\n",
      "24/08/23 16:16:56 WARN AttachDistributedSequenceExec: clean up cached RDD(78973) in AttachDistributedSequenceExec(848344)\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "result, groups_list = pac_worker.estimate_noise(math_df, v1=True)\n",
    "intermediate_time = datetime.datetime.now()\n",
    "\n",
    "pac_worker.release_pac_value(math_df, noise=result, groups_list=groups_list, threshold_value=20)\n",
    "end_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:11:55.878801\n",
      "0:00:01.800918\n"
     ]
    }
   ],
   "source": [
    "time_to_estimate_noise = intermediate_time - start_time\n",
    "print(time_to_estimate_noise)\n",
    "\n",
    "time_to_release_pac_value = end_time - intermediate_time\n",
    "print(time_to_release_pac_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "|guardian|avg(absences)|max(absences)|\n",
      "+--------+-------------+-------------+\n",
      "|  father|          0.0|          0.0|\n",
      "|  mother|          0.0|          0.0|\n",
      "|   other|          0.0|          0.0|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:3787: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [0. 4. 2.] + Noise = Noised: [-161.49424534268823, 10.341500735851202, 203.11546120594923]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "24/08/23 16:49:58 WARN AttachDistributedSequenceExec: clean up cached RDD(229904) in AttachDistributedSequenceExec(2501084)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|guardian|              count|\n",
      "+--------+-------------------+\n",
      "|  father|-161.49424534268823|\n",
      "|  mother| 10.341500735851202|\n",
      "|   other| 203.11546120594923|\n",
      "+--------+-------------------+\n",
      "\n",
      "Sample: [ 0.    0.   39.25 56.   31.   40.  ] + Noise = Noised: [-73.0973684428683, -153.7521251574019, 41.12384837094093, 133.60927974921583, -75.35582685223348, -137.90544191488195]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "/Users/chaitanyasuma/Library/Python/3.9/lib/python/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-------------------+\n",
      "|guardian|     avg(absences)|      max(absences)|\n",
      "+--------+------------------+-------------------+\n",
      "|  father| -73.0973684428683| -153.7521251574019|\n",
      "|  mother| 41.12384837094093| 133.60927974921583|\n",
      "|   other|-75.35582685223348|-137.90544191488195|\n",
      "+--------+------------------+-------------------+\n",
      "\n",
      "+--------+------------------+-------------------+\n",
      "|guardian|     avg(absences)|      max(absences)|\n",
      "+--------+------------------+-------------------+\n",
      "|  father| -73.0973684428683| -153.7521251574019|\n",
      "|  mother|               0.0|                0.0|\n",
      "|   other|-75.35582685223348|-137.90544191488195|\n",
      "+--------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/23 16:49:59 WARN AttachDistributedSequenceExec: clean up cached RDD(229996) in AttachDistributedSequenceExec(2502052)\n",
      "24/08/23 16:49:59 WARN AttachDistributedSequenceExec: clean up cached RDD(230016) in AttachDistributedSequenceExec(2502361)\n"
     ]
    }
   ],
   "source": [
    "start_time_ = datetime.datetime.now()\n",
    "result_, groups_list_ = pac_worker.estimate_noise(math_df, v2=True)\n",
    "intermediate_time_ = datetime.datetime.now()\n",
    "\n",
    "pac_worker.release_pac_value(math_df, noise=result_, groups_list=groups_list_, threshold_value=20)\n",
    "end_time_ = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:20:58.300983\n",
      "0:00:01.718860\n"
     ]
    }
   ],
   "source": [
    "time_to_estimate_noise_ = intermediate_time_ - start_time_\n",
    "print(time_to_estimate_noise_)\n",
    "\n",
    "time_to_release_pac_value_ = end_time_ - intermediate_time_\n",
    "print(time_to_release_pac_value_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
