{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAC-DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your generated samples\n",
    "ZIPFILE = f'./outputs/pac-duckdb-q1.zip'\n",
    "\n",
    "EXPERIMENT = 'pac-q1-e2e'\n",
    "OUTPUT_DIR = f'./outputs/{EXPERIMENT}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "import io\n",
    "\n",
    "def load_numpy_outputs_from_zip(zip_path):\n",
    "    output_np = []\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        for filename in zf.namelist():\n",
    "            with zf.open(filename) as file:\n",
    "                # if filename.endswith('.npy'):\n",
    "                #     arr = np.load(file, allow_pickle=True)\n",
    "                if filename.endswith('nparr.pkl'):\n",
    "                    arr = pickle.load(file)\n",
    "                else:\n",
    "                    # print(f\"Skipping files: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                output_np.append(arr)\n",
    "\n",
    "    return output_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "out_np = load_numpy_outputs_from_zip(ZIPFILE)\n",
    "\n",
    "print(f\"Loaded {len(out_np)} arrays\")\n",
    "print(type(out_np[0]), out_np[0].shape if hasattr(out_np[0], 'shape') else \"No shape\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(out_np[0]))\n",
    "# [array([[95549.5]]), \n",
    "# array([[95549.5]]), \n",
    "out_np = out_np[0]\n",
    "print(out_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import concurrent.futures\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pickle\n",
    "from numpy.random import laplace\n",
    "from functools import reduce\n",
    "import operator\n",
    "from IPython.display import display, HTML\n",
    "from datetime import date\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE = True\n",
    "USE_EVEN_NUMBER_OF_INPUT_ROWS = False\n",
    "SEED_RANDOM_NUMBER_GENERATOR = True\n",
    "\n",
    "SAMPLING_METHOD = 'poisson' # 'poisson' or 'half'\n",
    "\n",
    "if GENERATE:\n",
    "    print(\"GENERATE = True, so we will generate new samples.\")\n",
    "else:\n",
    "    print(\"GENERATE = False, so we will load saved output from files rather than recomputing.\")\n",
    "\n",
    "import os\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute PAC Noise\n",
    "def get_pac_noise_scale(out_np_raw: List[np.ndarray],\n",
    "                           max_mi: float = 1./4) -> np.ndarray:\n",
    "    if out_np_raw is None or len(out_np_raw) == 0:\n",
    "        raise ValueError(\"Input list out_np cannot be empty.\")\n",
    "    out_np = out_np_raw.copy()\n",
    "    # print(out_np)\n",
    "    dimensions: int = len(out_np[0])\n",
    "    print(f'The dimensions are - {dimensions}')\n",
    "\n",
    "    out_np_2darr = [np.atleast_2d(o) for o in out_np] # make sure all the DF -> np.ndarray conversions result in 2d arrays\n",
    "    print(out_np_2darr)\n",
    "    est_y: np.ndarray = np.stack(out_np_2darr, axis=-1).reshape(dimensions, len(out_np))  # shape (dimensions, samples)\n",
    "    print(f\"est_y.shape: {est_y.shape}\")\n",
    "    print(f\"est_y: {est_y}\")\n",
    "\n",
    "    # get the scale in each basis direction\n",
    "    fin_var: np.ndarray = np.var(est_y, axis=1)  # shape (dimensions,)\n",
    "    print(f\"fin_var: {fin_var}\")\n",
    "    # fin_var: np.ndarray = np.array([float(x) for x in fin_var], dtype=np.float64)\n",
    "    sqrt_total_var: np.floating[Any] = np.sum(np.sqrt(fin_var))\n",
    "    print(f\"sqrt_total_var: {sqrt_total_var}\")\n",
    "\n",
    "    # pac_noise: np.ndarray = (1./(2*max_mi)) * sqrt_total_var * np.sqrt(fin_var)  # scale of the PAC noise\n",
    "    # THE NEW NOISE CALCULATION ->\n",
    "    pac_noise: np.ndarray = (1./(2*max_mi)) * fin_var\n",
    "    print(f\"For mi={max_mi}, we should add noise from a normal distribution with scale...\")\n",
    "    print(f\"pac_noise: {pac_noise}\")\n",
    "    return pac_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_OPTIONS = [0.001248318631131131, 1/64, 1/32, 1/16, 1/4, 1., 2., 4., 16.]\n",
    "EXPERIMENTS = 1000\n",
    "SAMPLES = 1024\n",
    "OUTPUT_COLS = ['A_F_sum_qty', 'A_F_sum_base_price', 'A_F_sum_disc_price', 'A_F_sum_charge', 'A_F_avg_qty', 'A_F_avg_price', 'A_F_avg_disc', 'A_F_count_order',\n",
    "               'N_F_sum_qty', 'N_F_sum_base_price', 'N_F_sum_disc_price', 'N_F_sum_charge', 'N_F_avg_qty', 'N_F_avg_price', 'N_F_avg_disc', 'N_F_count_order',\n",
    "               'N_O_sum_qty', 'N_O_sum_base_price', 'N_O_sum_disc_price', 'N_O_sum_charge', 'N_O_avg_qty', 'N_O_avg_price', 'N_O_avg_disc', 'N_O_count_order',\n",
    "               'R_F_sum_qty', 'R_F_sum_base_price', 'R_F_sum_disc_price', 'R_F_sum_charge', 'R_F_avg_qty', 'R_F_avg_price', 'R_F_avg_disc', 'R_F_count_order'\n",
    "               ]\n",
    "\n",
    "if GENERATE:\n",
    "    experiment_results = []\n",
    "    saved_steps = []\n",
    "\n",
    "    print(f\"Generate samples... {len(out_np)} samples generated.\")\n",
    "\n",
    "    for mi in MI_OPTIONS:\n",
    "        print(\"Getting PAC Noise...\")\n",
    "        scale = get_pac_noise_scale(out_np, mi) # estimate the stability of the query\n",
    "        print(f\"mi={mi}, scale={scale}\")\n",
    "        \n",
    "        for e in range(EXPERIMENTS):\n",
    "            # for each PAC release at this MI, we will choose a sample from the pre-generated out_np list and add noise to it\n",
    "            steps = {\n",
    "                \"mi\": mi,\n",
    "                \"scale\": scale,\n",
    "            }\n",
    "\n",
    "            # choose our sample\n",
    "            chosen_index = np.random.choice(range(SAMPLES))\n",
    "            chosen_sample = out_np[chosen_index].copy()\n",
    "            steps[\"chosen_sample\"] = chosen_sample\n",
    "            \n",
    "            # add noise to it\n",
    "            # chosen_noise will also be an array\n",
    "            \n",
    "            chosen_noise = np.random.normal(loc=0, scale=np.sqrt(scale))\n",
    "            steps[\"chosen_noise\"] = chosen_noise\n",
    "            \n",
    "            print(f'Chosen Sample {chosen_sample}')\n",
    "            # chosen_sample = np.array([float(x) for x in chosen_sample], dtype=np.float64)\n",
    "            release = chosen_sample + chosen_noise # do_pac_and_release(out_np, mi, scale, chosen_index)\n",
    "\n",
    "            #print(f\"sample(#{chosen_index}):{chosen_sample} + noise:{chosen_noise} = {release}\")\n",
    "            steps[\"release\"] = release\n",
    "            #release[0] *= 2   # manually correct count = count * 2\n",
    "\n",
    "            # manually add sum as count * mean\n",
    "            #noisy_output = [noisy_output[0], noisy_output[0] * noisy_output[1], noisy_output[1]]\n",
    "            #chosen_sample = [chosen_sample[0], chosen_sample[0] * chosen_sample[1], chosen_sample[1]]\n",
    "            experiment_results.append([mi, *release])\n",
    "            saved_steps.append(steps)\n",
    "    \n",
    "    df = pd.DataFrame(experiment_results, columns=['mi', *OUTPUT_COLS])\n",
    "    \n",
    "    # Save the new data to outputs/...\n",
    "    df.to_parquet(f'{OUTPUT_DIR}/pac_results.parquet')\n",
    "    with open(f'{OUTPUT_DIR}/experiment_results.pkl', 'wb') as f:\n",
    "        pickle.dump(experiment_results, f)\n",
    "    with open(f'{OUTPUT_DIR}/saved_steps.pkl', 'wb') as f:\n",
    "        pickle.dump(saved_steps, f)\n",
    "else:\n",
    "    df = pq.read_table(f\"{OUTPUT_DIR}/pac_results.parquet\").to_pandas()\n",
    "\n",
    "    with open(f'{OUTPUT_DIR}/experiment_results.pkl', 'rb') as f:\n",
    "        experiment_results = pickle.load(f)\n",
    "    with open(f'{OUTPUT_DIR}/saved_steps.pkl', 'rb') as f:\n",
    "        saved_steps = pickle.load(f)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_steps_df_temp = pd.DataFrame(saved_steps)\n",
    "saved_steps_df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reconstruct Saved Steps\n",
    "# Get list of keys from the first dict, excluding 'mi' since it's not a list\n",
    "steps = [k for k in saved_steps[0].keys() if k not in ('mi')] # pull keys from saved_steps[0]\n",
    "\n",
    "saved_steps_df_temp = pd.DataFrame(saved_steps)\n",
    "\n",
    "# Create expanded columns using comprehension\n",
    "expanded = {\n",
    "    'mi': saved_steps_df_temp['mi'],\n",
    "    **{f'{step}_{col}': saved_steps_df_temp[step].str[i] \n",
    "        for step in steps\n",
    "        for i, col in enumerate(OUTPUT_COLS)}\n",
    "}\n",
    "\n",
    "# Create MultiIndex DataFrame using OUTPUT_COLS\n",
    "saved_steps_df = pd.DataFrame(expanded)\n",
    "saved_steps_df.columns = pd.MultiIndex.from_tuples([('mi',''), *[  # multiindex so that we can do things like saved_steps_df['release'][<aggregation>]\n",
    "    (step, col) for step in steps for col in OUTPUT_COLS\n",
    "]], names=[\"step\", \"query\"])\n",
    "saved_steps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_steps_df.groupby('mi').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_steps_df[saved_steps_df['mi'] == 1/4]['chosen_sample']['A_F_count_order'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('mean' in OUTPUT_COLS):\n",
    "    print(\"Mean of chosen_sample['mean'] for mi = 1/32\")\n",
    "    saved_steps_df[saved_steps_df['mi'] == 1/32]['chosen_sample']['mean'].hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
