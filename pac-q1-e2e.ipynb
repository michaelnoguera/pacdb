{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAC-DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your generated samples\n",
    "ZIPFILE = f'./outputs/pac-duckdb-q1.zip'\n",
    "\n",
    "EXPERIMENT = 'pac-q1-e2e'\n",
    "OUTPUT_DIR = f'./outputs/{EXPERIMENT}'\n",
    "\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([20, 16, 6, 7, 17, 21, 10, 0, 26, 30, 31, 27, 1, 11, 2, 28, 12, 24, 25, 13, 29, 3, 8, 22, 18, 4, 14, 15, 5, 19, 23, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the json input files\n",
    "# {'col': column name as string,\n",
    "#  'row': group-by column values as dict (effectively a row id),\n",
    "#  'values': [ 1000x values ] }\n",
    "alldata = {}\n",
    "with zipfile.ZipFile(ZIPFILE, 'r') as zf:\n",
    "    for filename in zf.namelist():\n",
    "        if filename.startswith('json/') and filename.endswith('.json'):\n",
    "            #print(filename)\n",
    "            with zf.open(filename) as f:\n",
    "                filenumber = int(filename.split('/')[-1].split('.')[0])\n",
    "                data = json.load(f)\n",
    "                d: dict = data[0]\n",
    "                alldata[filenumber] = d\n",
    "alldata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(alldata)):\n",
    "    # if values is not a numeric list, make it one\n",
    "    npvalues = np.array(alldata[i]['values'])\n",
    "    if not np.issubdtype(npvalues.dtype, np.number):\n",
    "        # try to convert to numeric\n",
    "        try:\n",
    "            npvalues = pd.to_numeric(npvalues)\n",
    "        except ValueError:\n",
    "            print(f'Could not convert values in {i} to numeric')\n",
    "            continue\n",
    "    alldata[i]['values'] = npvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'col': 'sum_qty',\n",
       " 'row': {'l_returnflag': 'A', 'l_linestatus': 'F'},\n",
       " 'values': array([3757862., 3806350., 3742618., ..., 3771744., 3727774., 3789002.])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1024 arrays\n",
      "<class 'numpy.float64'> ()\n"
     ]
    }
   ],
   "source": [
    "out_np = alldata[0]['values']\n",
    "\n",
    "print(f\"Loaded {len(out_np)} arrays\")\n",
    "print(type(out_np[0]), out_np[0].shape if hasattr(out_np[0], 'shape') else \"No shape\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_np\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# [array([[95549.5]]), \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# array([[95549.5]]), \u001b[39;00m\n\u001b[1;32m      4\u001b[0m out_np \u001b[38;5;241m=\u001b[39m out_np[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "print(len(out_np[0]))\n",
    "# [array([[95549.5]]), \n",
    "# array([[95549.5]]), \n",
    "out_np = out_np[0]\n",
    "print(out_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import concurrent.futures\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pickle\n",
    "from numpy.random import laplace\n",
    "from functools import reduce\n",
    "import operator\n",
    "from IPython.display import display, HTML\n",
    "from datetime import date\n",
    "from scipy import special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE = True, so we will generate new samples.\n"
     ]
    }
   ],
   "source": [
    "GENERATE = True\n",
    "USE_EVEN_NUMBER_OF_INPUT_ROWS = False\n",
    "SEED_RANDOM_NUMBER_GENERATOR = True\n",
    "\n",
    "SAMPLING_METHOD = 'poisson' # 'poisson' or 'half'\n",
    "\n",
    "if GENERATE:\n",
    "    print(\"GENERATE = True, so we will generate new samples.\")\n",
    "else:\n",
    "    print(\"GENERATE = False, so we will load saved output from files rather than recomputing.\")\n",
    "\n",
    "import os\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute PAC Noise\n",
    "def get_pac_noise_scale(out_np_raw: List[np.ndarray],\n",
    "                           max_mi: float = 1./4) -> np.ndarray:\n",
    "    if out_np_raw is None or len(out_np_raw) == 0:\n",
    "        raise ValueError(\"Input list out_np cannot be empty.\")\n",
    "    out_np = out_np_raw.copy()\n",
    "    # print(out_np)\n",
    "    dimensions: int = len(out_np[0])\n",
    "    print(f'The dimensions are - {dimensions}')\n",
    "\n",
    "    out_np_2darr = [np.atleast_2d(o) for o in out_np] # make sure all the DF -> np.ndarray conversions result in 2d arrays\n",
    "    print(out_np_2darr)\n",
    "    est_y: np.ndarray = np.stack(out_np_2darr, axis=-1).reshape(dimensions, len(out_np))  # shape (dimensions, samples)\n",
    "    print(f\"est_y.shape: {est_y.shape}\")\n",
    "    print(f\"est_y: {est_y}\")\n",
    "\n",
    "    # get the scale in each basis direction\n",
    "    fin_var: np.ndarray = np.var(est_y, axis=1)  # shape (dimensions,)\n",
    "    print(f\"fin_var: {fin_var}\")\n",
    "    # fin_var: np.ndarray = np.array([float(x) for x in fin_var], dtype=np.float64)\n",
    "    sqrt_total_var: np.floating[Any] = np.sum(np.sqrt(fin_var))\n",
    "    print(f\"sqrt_total_var: {sqrt_total_var}\")\n",
    "\n",
    "    # pac_noise: np.ndarray = (1./(2*max_mi)) * sqrt_total_var * np.sqrt(fin_var)  # scale of the PAC noise\n",
    "    # THE NEW NOISE CALCULATION ->\n",
    "    pac_noise: np.ndarray = (1./(2*max_mi)) * fin_var\n",
    "    print(f\"For mi={max_mi}, we should add noise from a normal distribution with scale...\")\n",
    "    print(f\"pac_noise: {pac_noise}\")\n",
    "    return pac_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate samples... 1024 samples generated.\n",
      "Getting PAC Noise...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mi \u001b[38;5;129;01min\u001b[39;00m MI_OPTIONS:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetting PAC Noise...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     scale \u001b[38;5;241m=\u001b[39m \u001b[43mget_pac_noise_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# estimate the stability of the query\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmi=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, scale=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EXPERIMENTS):\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# for each PAC release at this MI, we will choose a sample from the pre-generated out_np list and add noise to it\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m, in \u001b[0;36mget_pac_noise_scale\u001b[0;34m(out_np_raw, max_mi)\u001b[0m\n\u001b[1;32m      6\u001b[0m out_np \u001b[38;5;241m=\u001b[39m out_np_raw\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(out_np)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m dimensions: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_np\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe dimensions are - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdimensions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m out_np_2darr \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39matleast_2d(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m out_np] \u001b[38;5;66;03m# make sure all the DF -> np.ndarray conversions result in 2d arrays\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "MI_OPTIONS = [0.001248318631131131, 1/64, 1/32, 1/16, 1/4, 1., 2., 4., 16.]\n",
    "EXPERIMENTS = 1000\n",
    "SAMPLES = len(out_np)\n",
    "\n",
    "if GENERATE:\n",
    "    experiment_results = []\n",
    "    saved_steps = []\n",
    "\n",
    "    print(f\"Generate samples... {len(out_np)} samples generated.\")\n",
    "\n",
    "    for mi in MI_OPTIONS:\n",
    "        print(\"Getting PAC Noise...\")\n",
    "        scale = get_pac_noise_scale(out_np, mi) # estimate the stability of the query\n",
    "        print(f\"mi={mi}, scale={scale}\")\n",
    "        \n",
    "        for e in range(EXPERIMENTS):\n",
    "            # for each PAC release at this MI, we will choose a sample from the pre-generated out_np list and add noise to it\n",
    "            steps = {\n",
    "                \"mi\": mi,\n",
    "                \"scale\": scale,\n",
    "            }\n",
    "\n",
    "            # choose our sample\n",
    "            chosen_index = np.random.choice(range(SAMPLES))\n",
    "            chosen_sample = out_np[chosen_index].copy()\n",
    "            steps[\"chosen_sample\"] = chosen_sample\n",
    "            \n",
    "            # add noise to it\n",
    "            # chosen_noise will also be an array\n",
    "            \n",
    "            chosen_noise = np.random.normal(loc=0, scale=np.sqrt(scale))\n",
    "            steps[\"chosen_noise\"] = chosen_noise\n",
    "            \n",
    "            print(f'Chosen Sample {chosen_sample}')\n",
    "            # chosen_sample = np.array([float(x) for x in chosen_sample], dtype=np.float64)\n",
    "            release = chosen_sample + chosen_noise # do_pac_and_release(out_np, mi, scale, chosen_index)\n",
    "\n",
    "            #print(f\"sample(#{chosen_index}):{chosen_sample} + noise:{chosen_noise} = {release}\")\n",
    "            steps[\"release\"] = release\n",
    "            #release[0] *= 2   # manually correct count = count * 2\n",
    "\n",
    "            # manually add sum as count * mean\n",
    "            #noisy_output = [noisy_output[0], noisy_output[0] * noisy_output[1], noisy_output[1]]\n",
    "            #chosen_sample = [chosen_sample[0], chosen_sample[0] * chosen_sample[1], chosen_sample[1]]\n",
    "            experiment_results.append([mi, *release])\n",
    "            saved_steps.append(steps)\n",
    "    \n",
    "    df = pd.DataFrame(experiment_results, columns=['mi', *OUTPUT_COLS])\n",
    "    \n",
    "    # Save the new data to outputs/...\n",
    "    df.to_parquet(f'{OUTPUT_DIR}/pac_results.parquet')\n",
    "    with open(f'{OUTPUT_DIR}/experiment_results.pkl', 'wb') as f:\n",
    "        pickle.dump(experiment_results, f)\n",
    "    with open(f'{OUTPUT_DIR}/saved_steps.pkl', 'wb') as f:\n",
    "        pickle.dump(saved_steps, f)\n",
    "else:\n",
    "    df = pq.read_table(f\"{OUTPUT_DIR}/pac_results.parquet\").to_pandas()\n",
    "\n",
    "    with open(f'{OUTPUT_DIR}/experiment_results.pkl', 'rb') as f:\n",
    "        experiment_results = pickle.load(f)\n",
    "    with open(f'{OUTPUT_DIR}/saved_steps.pkl', 'rb') as f:\n",
    "        saved_steps = pickle.load(f)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_steps_df_temp = pd.DataFrame(saved_steps)\n",
    "saved_steps_df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reconstruct Saved Steps\n",
    "# Get list of keys from the first dict, excluding 'mi' since it's not a list\n",
    "steps = [k for k in saved_steps[0].keys() if k not in ('mi')] # pull keys from saved_steps[0]\n",
    "\n",
    "saved_steps_df_temp = pd.DataFrame(saved_steps)\n",
    "\n",
    "# Create expanded columns using comprehension\n",
    "expanded = {\n",
    "    'mi': saved_steps_df_temp['mi'],\n",
    "    **{f'{step}_{col}': saved_steps_df_temp[step].str[i] \n",
    "        for step in steps\n",
    "        for i, col in enumerate(OUTPUT_COLS)}\n",
    "}\n",
    "\n",
    "# Create MultiIndex DataFrame using OUTPUT_COLS\n",
    "saved_steps_df = pd.DataFrame(expanded)\n",
    "saved_steps_df.columns = pd.MultiIndex.from_tuples([('mi',''), *[  # multiindex so that we can do things like saved_steps_df['release'][<aggregation>]\n",
    "    (step, col) for step in steps for col in OUTPUT_COLS\n",
    "]], names=[\"step\", \"query\"])\n",
    "saved_steps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_steps_df.groupby('mi').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_steps_df[saved_steps_df['mi'] == 1/4]['chosen_sample']['A_F_count_order'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('mean' in OUTPUT_COLS):\n",
    "    print(\"Mean of chosen_sample['mean'] for mi = 1/32\")\n",
    "    saved_steps_df[saved_steps_df['mi'] == 1/32]['chosen_sample']['mean'].hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
