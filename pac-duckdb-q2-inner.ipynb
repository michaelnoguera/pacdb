{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB Notebook\n",
    "\n",
    "This notebook generates a bunch of raw outputs, without applying PAC, to be consumed by a second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE = False, so we will load saved output from files rather than recomputing.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "EXPERIMENT = 'pac-duckdb-q2-inner'\n",
    "OUTPUT_DIR = f'./outputs/{EXPERIMENT}'\n",
    "GENERATE = False\n",
    "USE_EVEN_NUMBER_OF_INPUT_ROWS = False\n",
    "\n",
    "if GENERATE:\n",
    "    print(\"GENERATE = True, so we will generate new samples.\")\n",
    "else:\n",
    "    print(\"GENERATE = False, so we will load saved output from files rather than recomputing.\")\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import duckdb\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "\n",
    "# duckdb load data/tpch/tpch.duckdb\n",
    "#con = duckdb.connect(database='data/tpch/tpch.duckdb', read_only=True)\n",
    "con = duckdb.connect(database=':memory:')\n",
    "tables = [\"customer\", \"lineitem\", \"nation\", \"orders\", \"part\", \"partsupp\", \"region\", \"supplier\"]\n",
    "#tables = [\"lineitem\", \"orders\"]\n",
    "for t in tables:\n",
    "    con.execute(f\"CREATE TABLE {t} AS SELECT * FROM 'data/tpch/{t}.parquet'\")\n",
    "\n",
    "lineitem_df = con.execute(\"SELECT * FROM lineitem\").fetchdf()\n",
    "orders_df = con.execute(\"SELECT * FROM orders\").fetchdf()\n",
    "\n",
    "row_count = lineitem_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the table of random samples\n",
    "# to use, join it with the lineitem table (for specific sample # s) and filter to just the\n",
    "# rows where random_binary = 1.0\n",
    "# This will give us a 50% sample of the lineitem table for each sample # s\n",
    "\n",
    "SAMPLES = 1024\n",
    "assert SAMPLES % 2 == 0, \"SAMPLES must be even to create complementary samples.\"\n",
    "\n",
    "random_samples = con.execute(f\"\"\"\n",
    "DROP TABLE IF EXISTS random_samples;\n",
    "\n",
    "CREATE TABLE random_samples AS\n",
    "WITH sample_numbers AS MATERIALIZED (\n",
    "    SELECT range AS sample_id FROM range({SAMPLES//2})\n",
    "), random_values AS MATERIALIZED (\n",
    "    SELECT \n",
    "        sample_numbers.sample_id,\n",
    "        supplier.rowid AS row_id,\n",
    "        (RANDOM() > 0.5)::BOOLEAN AS random_binary\n",
    "    FROM sample_numbers\n",
    "    JOIN supplier ON TRUE  -- Cross join to duplicate rows for each sample\n",
    ")\n",
    "SELECT\n",
    "    sample_id,\n",
    "    row_id,\n",
    "    random_binary\n",
    "FROM random_values\n",
    "UNION ALL\n",
    "SELECT -- select the complementary samples too\n",
    "    ({SAMPLES//2}) + sample_id,\n",
    "    row_id,\n",
    "    NOT random_binary  -- Inverse the random_binary to get the complementary sample\n",
    "FROM random_values\n",
    "ORDER BY sample_id, row_id;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_sizes = con.execute(\"\"\"\n",
    "#SELECT sample_id, SUM(random_binary) AS sample_size\n",
    "#FROM random_samples\n",
    "#GROUP BY sample_id;\n",
    "#\"\"\").pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The randomness of what rows are chosen is saved to disk in `random_binary.json`. For each sample #, there is an array with one entry per row, where 1 means the row was chosen and 0 means it was not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(f\"\"\"\n",
    "SELECT sample_id, array_agg(random_binary::TINYINT) as random_binary\n",
    "FROM random_samples\n",
    "GROUP BY sample_id;\n",
    "\"\"\").pl().write_json(f\"{OUTPUT_DIR}/random_binary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query is specified as a prepared statement. We will then execute it once per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (33, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>p_partkey</th><th>rank</th><th>ps_supplycost</th><th>s_name</th></tr><tr><td>i64</td><td>i64</td><td>decimal[15,2]</td><td>str</td></tr></thead><tbody><tr><td>1015</td><td>1</td><td>795.39</td><td>&quot;Supplier#000000016&quot;</td></tr><tr><td>1015</td><td>2</td><td>929.07</td><td>&quot;Supplier#000000769&quot;</td></tr><tr><td>2156</td><td>1</td><td>218.40</td><td>&quot;Supplier#000000409&quot;</td></tr><tr><td>3563</td><td>1</td><td>346.02</td><td>&quot;Supplier#000000070&quot;</td></tr><tr><td>3563</td><td>2</td><td>651.31</td><td>&quot;Supplier#000000323&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>17242</td><td>1</td><td>191.95</td><td>&quot;Supplier#000000510&quot;</td></tr><tr><td>17242</td><td>2</td><td>954.01</td><td>&quot;Supplier#000000243&quot;</td></tr><tr><td>17268</td><td>1</td><td>738.28</td><td>&quot;Supplier#000000070&quot;</td></tr><tr><td>18103</td><td>1</td><td>515.38</td><td>&quot;Supplier#000000104&quot;</td></tr><tr><td>18344</td><td>1</td><td>712.75</td><td>&quot;Supplier#000000149&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (33, 4)\n",
       "┌───────────┬──────┬───────────────┬────────────────────┐\n",
       "│ p_partkey ┆ rank ┆ ps_supplycost ┆ s_name             │\n",
       "│ ---       ┆ ---  ┆ ---           ┆ ---                │\n",
       "│ i64       ┆ i64  ┆ decimal[15,2] ┆ str                │\n",
       "╞═══════════╪══════╪═══════════════╪════════════════════╡\n",
       "│ 1015      ┆ 1    ┆ 795.39        ┆ Supplier#000000016 │\n",
       "│ 1015      ┆ 2    ┆ 929.07        ┆ Supplier#000000769 │\n",
       "│ 2156      ┆ 1    ┆ 218.40        ┆ Supplier#000000409 │\n",
       "│ 3563      ┆ 1    ┆ 346.02        ┆ Supplier#000000070 │\n",
       "│ 3563      ┆ 2    ┆ 651.31        ┆ Supplier#000000323 │\n",
       "│ …         ┆ …    ┆ …             ┆ …                  │\n",
       "│ 17242     ┆ 1    ┆ 191.95        ┆ Supplier#000000510 │\n",
       "│ 17242     ┆ 2    ┆ 954.01        ┆ Supplier#000000243 │\n",
       "│ 17268     ┆ 1    ┆ 738.28        ┆ Supplier#000000070 │\n",
       "│ 18103     ┆ 1    ┆ 515.38        ┆ Supplier#000000104 │\n",
       "│ 18344     ┆ 1    ┆ 712.75        ┆ Supplier#000000149 │\n",
       "└───────────┴──────┴───────────────┴────────────────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query\n",
    "con.execute(\"\"\"\n",
    "DEALLOCATE PREPARE run_query;\n",
    "\n",
    "PREPARE run_query AS \n",
    "SELECT\n",
    "  base.p_partkey,\n",
    "  RANK() OVER (\n",
    "    PARTITION BY base.p_partkey\n",
    "    ORDER BY base.ps_supplycost\n",
    "  ) AS rank,\n",
    "  base.ps_supplycost,\n",
    "  base.s_name,\n",
    "FROM (\n",
    "  SELECT\n",
    "    p_partkey,\n",
    "    ps_supplycost,\n",
    "    s_name\n",
    "  FROM\n",
    "    partsupp\n",
    "  JOIN part ON part.p_partkey = partsupp.ps_partkey\n",
    "  JOIN supplier ON supplier.s_suppkey = partsupp.ps_suppkey\n",
    "  JOIN nation ON supplier.s_nationkey = nation.n_nationkey\n",
    "  JOIN region ON nation.n_regionkey = region.r_regionkey\n",
    "  JOIN random_samples AS rs ON rs.row_id = supplier.rowid\n",
    "  WHERE\n",
    "    p_size = 15\n",
    "    AND p_type LIKE '%BRASS'\n",
    "    AND r_name = 'EUROPE'\n",
    "    AND rs.random_binary = TRUE\n",
    "    AND rs.sample_id = $sample\n",
    ") AS base\n",
    "ORDER BY\n",
    "  p_partkey,\n",
    "  rank;\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"EXECUTE run_query(sample := {0});\").pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate the query result for each sample. The query output of each sample is saved to disk in multiple formats:\n",
    "- `csv` contains one file per sample, with the table written in CSV format. This does not preserve data type information.\n",
    "- `parquet` contains one file per sample, with the table written in Parquet format. This preserves data type information as apache arrow converted types.\n",
    "- `dfs.pkl` contains the python list of polars dataframes in a binary format. This could be used to resume the notebook with the exact same previously-used randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all the queries\n",
    "dfs: List[pl.DataFrame] = []\n",
    "for s in range(SAMPLES):\n",
    "    dfs.append(con.execute(f\"EXECUTE run_query(sample := {s});\").pl())\n",
    "\n",
    "# Save the results to disk\n",
    "os.makedirs(f\"{OUTPUT_DIR}/csv\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/parquet\", exist_ok=True)\n",
    "for i, df in enumerate(dfs):\n",
    "    df.write_csv(f\"{OUTPUT_DIR}/csv/sample_{i}.csv\")\n",
    "    df.write_parquet(f\"{OUTPUT_DIR}/parquet/sample_{i}.parquet\")\n",
    "with open(f\"{OUTPUT_DIR}/dfs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dfs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples have been generated and stored in `outputs/{OUTPUT_DIR}/csv/sample_{i}.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p_partkey', 'rank', 'ps_supplycost', 's_name']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output schema: Schema([('p_partkey', Int64), ('rank', Int64), ('ps_supplycost', Decimal(precision=15, scale=2)), ('s_name', String)])\n",
      "Output shape: (33, 4)\n"
     ]
    }
   ],
   "source": [
    "INDEX_COLS = []\n",
    "OUTPUT_COLS = ['p_partkey', 'rank', 'ps_supplycost', 's_name']\n",
    "OUTPUT_SCHEMA = dfs[0].select(OUTPUT_COLS).collect_schema()\n",
    "OUTPUT_SHAPE = dfs[0].select(OUTPUT_COLS).to_numpy().shape\n",
    "with open(f\"{OUTPUT_DIR}/schema.txt\", \"w\") as f:\n",
    "    f.write(str(OUTPUT_SCHEMA))\n",
    "print(f\"Output schema: {OUTPUT_SCHEMA}\")\n",
    "print(f\"Output shape: {OUTPUT_SHAPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (33, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>p_partkey</th><th>rank</th><th>ps_supplycost</th><th>s_name</th></tr><tr><td>i64</td><td>i64</td><td>decimal[15,2]</td><td>str</td></tr></thead><tbody><tr><td>1015</td><td>1</td><td>795.39</td><td>&quot;Supplier#000000016&quot;</td></tr><tr><td>1015</td><td>2</td><td>929.07</td><td>&quot;Supplier#000000769&quot;</td></tr><tr><td>2156</td><td>1</td><td>218.40</td><td>&quot;Supplier#000000409&quot;</td></tr><tr><td>3563</td><td>1</td><td>346.02</td><td>&quot;Supplier#000000070&quot;</td></tr><tr><td>3563</td><td>2</td><td>651.31</td><td>&quot;Supplier#000000323&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>17242</td><td>1</td><td>191.95</td><td>&quot;Supplier#000000510&quot;</td></tr><tr><td>17242</td><td>2</td><td>954.01</td><td>&quot;Supplier#000000243&quot;</td></tr><tr><td>17268</td><td>1</td><td>738.28</td><td>&quot;Supplier#000000070&quot;</td></tr><tr><td>18103</td><td>1</td><td>515.38</td><td>&quot;Supplier#000000104&quot;</td></tr><tr><td>18344</td><td>1</td><td>712.75</td><td>&quot;Supplier#000000149&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (33, 4)\n",
       "┌───────────┬──────┬───────────────┬────────────────────┐\n",
       "│ p_partkey ┆ rank ┆ ps_supplycost ┆ s_name             │\n",
       "│ ---       ┆ ---  ┆ ---           ┆ ---                │\n",
       "│ i64       ┆ i64  ┆ decimal[15,2] ┆ str                │\n",
       "╞═══════════╪══════╪═══════════════╪════════════════════╡\n",
       "│ 1015      ┆ 1    ┆ 795.39        ┆ Supplier#000000016 │\n",
       "│ 1015      ┆ 2    ┆ 929.07        ┆ Supplier#000000769 │\n",
       "│ 2156      ┆ 1    ┆ 218.40        ┆ Supplier#000000409 │\n",
       "│ 3563      ┆ 1    ┆ 346.02        ┆ Supplier#000000070 │\n",
       "│ 3563      ┆ 2    ┆ 651.31        ┆ Supplier#000000323 │\n",
       "│ …         ┆ …    ┆ …             ┆ …                  │\n",
       "│ 17242     ┆ 1    ┆ 191.95        ┆ Supplier#000000510 │\n",
       "│ 17242     ┆ 2    ┆ 954.01        ┆ Supplier#000000243 │\n",
       "│ 17268     ┆ 1    ┆ 738.28        ┆ Supplier#000000070 │\n",
       "│ 18103     ┆ 1    ┆ 515.38        ┆ Supplier#000000104 │\n",
       "│ 18344     ┆ 1    ┆ 712.75        ┆ Supplier#000000149 │\n",
       "└───────────┴──────┴───────────────┴────────────────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0].select(OUTPUT_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyify(df: pl.DataFrame) -> np.ndarray:\n",
    "    return df.select(OUTPUT_COLS).to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (33, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>p_partkey</th><th>rank</th><th>ps_supplycost</th><th>s_name</th></tr><tr><td>i64</td><td>i64</td><td>decimal[15,2]</td><td>str</td></tr></thead><tbody><tr><td>1015</td><td>1</td><td>795.39</td><td>&quot;Supplier#000000016&quot;</td></tr><tr><td>1015</td><td>2</td><td>929.07</td><td>&quot;Supplier#000000769&quot;</td></tr><tr><td>2156</td><td>1</td><td>218.40</td><td>&quot;Supplier#000000409&quot;</td></tr><tr><td>3563</td><td>1</td><td>346.02</td><td>&quot;Supplier#000000070&quot;</td></tr><tr><td>3563</td><td>2</td><td>651.31</td><td>&quot;Supplier#000000323&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>17242</td><td>1</td><td>191.95</td><td>&quot;Supplier#000000510&quot;</td></tr><tr><td>17242</td><td>2</td><td>954.01</td><td>&quot;Supplier#000000243&quot;</td></tr><tr><td>17268</td><td>1</td><td>738.28</td><td>&quot;Supplier#000000070&quot;</td></tr><tr><td>18103</td><td>1</td><td>515.38</td><td>&quot;Supplier#000000104&quot;</td></tr><tr><td>18344</td><td>1</td><td>712.75</td><td>&quot;Supplier#000000149&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (33, 4)\n",
       "┌───────────┬──────┬───────────────┬────────────────────┐\n",
       "│ p_partkey ┆ rank ┆ ps_supplycost ┆ s_name             │\n",
       "│ ---       ┆ ---  ┆ ---           ┆ ---                │\n",
       "│ i64       ┆ i64  ┆ decimal[15,2] ┆ str                │\n",
       "╞═══════════╪══════╪═══════════════╪════════════════════╡\n",
       "│ 1015      ┆ 1    ┆ 795.39        ┆ Supplier#000000016 │\n",
       "│ 1015      ┆ 2    ┆ 929.07        ┆ Supplier#000000769 │\n",
       "│ 2156      ┆ 1    ┆ 218.40        ┆ Supplier#000000409 │\n",
       "│ 3563      ┆ 1    ┆ 346.02        ┆ Supplier#000000070 │\n",
       "│ 3563      ┆ 2    ┆ 651.31        ┆ Supplier#000000323 │\n",
       "│ …         ┆ …    ┆ …             ┆ …                  │\n",
       "│ 17242     ┆ 1    ┆ 191.95        ┆ Supplier#000000510 │\n",
       "│ 17242     ┆ 2    ┆ 954.01        ┆ Supplier#000000243 │\n",
       "│ 17268     ┆ 1    ┆ 738.28        ┆ Supplier#000000070 │\n",
       "│ 18103     ┆ 1    ┆ 515.38        ┆ Supplier#000000104 │\n",
       "│ 18344     ┆ 1    ┆ 712.75        ┆ Supplier#000000149 │\n",
       "└───────────┴──────┴───────────────┴────────────────────┘"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablify(arr: np.ndarray) -> pl.DataFrame:\n",
    "    global OUTPUT_SHAPE, OUTPUT_SCHEMA\n",
    "    return dfs[0].update( # put values back into the original dataframe\n",
    "        pl.DataFrame(\n",
    "            arr.reshape(OUTPUT_SHAPE), # reshape to the original shape\n",
    "            schema=OUTPUT_SCHEMA # coerce numpy array to the correct schema\n",
    "        ) # index cols will be left unchanged (not updated b/c we only update output_cols)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples in numpy format are saved to disk in a variety of ways, all of which contain the same data:\n",
    "- `npy` contains arrays saved in the Numpy format. See https://numpy.org/doc/stable/reference/generated/numpy.lib.format.html\n",
    "- `npcsv` contains numpy arrays saved in the CSV format. These are 1D arrays of whatever data type (probably float) is in the table.\n",
    "- `nparr.npz` contains all the numpy arrays saved in the Numpy format for saving multiple arrays in one file. See https://numpy.org/doc/stable/reference/generated/numpy.savez.html\n",
    "- `nparr.pkl` contains the python list of numpy arrays in a binary format, if you don't want to use the numpy format for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrames to numpy arrays\n",
    "nparr = [numpyify(df) for df in dfs]\n",
    "\n",
    "# Save the numpy arrays to disk\n",
    "os.makedirs(f\"{OUTPUT_DIR}/npy\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/npcsv\", exist_ok=True)\n",
    "for i, arr in enumerate(nparr):\n",
    "    np.save(f\"{OUTPUT_DIR}/npy/arr_{i}.npy\", arr)\n",
    "for i, arr in enumerate(nparr):\n",
    "    with open(f\"{OUTPUT_DIR}/npcsv/arr_{i}.csv\", \"w\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(arr)\n",
    "np.savez(f\"{OUTPUT_DIR}/nparr.npz\", *nparr)\n",
    "with open(f\"{OUTPUT_DIR}/nparr.pkl\", \"wb\") as f:\n",
    "    pickle.dump(nparr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to load the `npz` file into an array of 'samples' where the samples are each a 1d numpy array.\n",
    "```python\n",
    "test = np.load(f\"{OUTPUT_DIR}/nparr.npz\")\n",
    "npsamples = [test[f'arr_{i}'] for i in range(SAMPLES)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.load(f\"{OUTPUT_DIR}/nparr.npz\", allow_pickle=True)\n",
    "npsamples = [test[f'arr_{i}'] for i in range(SAMPLES)]\n",
    "npsamples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/michael/projects/dpdb/pacdb/outputs/pac-duckdb-q2-inner.zip'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip the OUTPUT_DIR\n",
    "import shutil\n",
    "shutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('p_partkey', Int64),\n",
       "        ('rank', Int64),\n",
       "        ('ps_supplycost', Decimal(precision=15, scale=2)),\n",
       "        ('s_name', String)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0].schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>p_partkey</th><th>rank</th><th>ps_supplycost</th><th>s_name</th></tr><tr><td>i64</td><td>i64</td><td>decimal[15,2]</td><td>str</td></tr></thead><tbody><tr><td>1015</td><td>2</td><td>929.07</td><td>&quot;Supplier#000000769&quot;</td></tr><tr><td>3563</td><td>2</td><td>651.31</td><td>&quot;Supplier#000000323&quot;</td></tr><tr><td>10956</td><td>2</td><td>893.82</td><td>&quot;Supplier#000000957&quot;</td></tr><tr><td>13120</td><td>2</td><td>535.02</td><td>&quot;Supplier#000000121&quot;</td></tr><tr><td>17242</td><td>2</td><td>954.01</td><td>&quot;Supplier#000000243&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌───────────┬──────┬───────────────┬────────────────────┐\n",
       "│ p_partkey ┆ rank ┆ ps_supplycost ┆ s_name             │\n",
       "│ ---       ┆ ---  ┆ ---           ┆ ---                │\n",
       "│ i64       ┆ i64  ┆ decimal[15,2] ┆ str                │\n",
       "╞═══════════╪══════╪═══════════════╪════════════════════╡\n",
       "│ 1015      ┆ 2    ┆ 929.07        ┆ Supplier#000000769 │\n",
       "│ 3563      ┆ 2    ┆ 651.31        ┆ Supplier#000000323 │\n",
       "│ 10956     ┆ 2    ┆ 893.82        ┆ Supplier#000000957 │\n",
       "│ 13120     ┆ 2    ┆ 535.02        ┆ Supplier#000000121 │\n",
       "│ 17242     ┆ 2    ┆ 954.01        ┆ Supplier#000000243 │\n",
       "└───────────┴──────┴───────────────┴────────────────────┘"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0].filter(pl.col('rank') == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{OUTPUT_DIR}/split/csv\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/split/npy\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/split/npcsv\", exist_ok=True)\n",
    "for i, df in enumerate(dfs):\n",
    "    all_ranks_present = df.select(pl.col('rank')).unique().sort('rank').to_numpy().flatten()\n",
    "    # save a csv of format split/sample_{i}_rank_{r}.csv\n",
    "    for r in all_ranks_present:\n",
    "        df.filter(pl.col('rank') == r).write_csv(f\"{OUTPUT_DIR}/split/csv/sample_{i}_rank_{r}.csv\")\n",
    "        # npy\n",
    "        np.save(f\"{OUTPUT_DIR}/split/npy/sample_{i}_rank_{r}.npy\", numpyify(df.filter(pl.col('rank') == r)))\n",
    "        # npcsv\n",
    "        with open(f\"{OUTPUT_DIR}/split/npcsv/sample_{i}_rank_{r}.csv\", \"w\", newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(numpyify(df.filter(pl.col('rank') == r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/michael/projects/dpdb/pacdb/outputs/pac-duckdb-q2-inner.zip'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
