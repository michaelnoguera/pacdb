{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified TPC-H Q4 Microbenchmark\n",
    "This file is adapted to use numpy only without Spark.\n",
    "\n",
    "```sql\n",
    "SELECT count(*) AS order_count\n",
    "FROM orders\n",
    "WHERE o_orderpriority = '1-URGENT'\n",
    "AND o_orderdate >= CAST('1993-07-01' AS date)\n",
    "AND o_orderdate < CAST('1993-10-01' AS date)\n",
    "AND EXISTS (\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        lineitem\n",
    "    WHERE\n",
    "        l_orderkey = o_orderkey\n",
    "        AND l_commitdate < l_receiptdate);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'pac-q4'\n",
    "OUTPUT_DIR = f'./outputs/{EXPERIMENT}'\n",
    "GENERATE = True\n",
    "USE_EVEN_NUMBER_OF_INPUT_ROWS = True\n",
    "SEED_RANDOM_NUMBER_GENERATOR = True\n",
    "\n",
    "SAMPLING_METHOD = 'half' # 'poisson' or 'half'\n",
    "\n",
    "if GENERATE:\n",
    "    print(\"GENERATE = True, so we will generate new samples.\")\n",
    "else:\n",
    "    print(\"GENERATE = False, so we will load saved output from files rather than recomputing.\")\n",
    "\n",
    "import os\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "if SEED_RANDOM_NUMBER_GENERATOR:\n",
    "    np.random.seed(0)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import concurrent.futures\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pickle\n",
    "from numpy.random import laplace\n",
    "from functools import reduce\n",
    "import operator\n",
    "from IPython.display import display, HTML\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mayuri's conversion functions between DP epsilon and PAC MI using posterior advantage for equivalence\n",
    "def calc_posterior(mi, prior=0.5, prec = 100000):\n",
    "    test_vals = [x / prec for x in range(1, prec)]\n",
    "    max_t = None\n",
    "    for t in test_vals:\n",
    "        if t*np.log(t/prior)+(1-t)*np.log((1-t)/(1-prior)) <= mi:\n",
    "            if  max_t is None or t > max_t:\n",
    "                max_t = t\n",
    "    return max_t\n",
    "\n",
    "def dp_epsilon_to_posterior_success(epsilon):\n",
    "    return 1 - 1./(1+np.exp(epsilon))\n",
    "\n",
    "def dp_ps_to_epsilon(ps):\n",
    "    return np.log(ps / (1-ps))\n",
    "\n",
    "# example usage:\n",
    "# dp_ps_to_epsilon(calc_posterior(1/256.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Setup\n",
    "#por_df = pq.read_table(f\"./data/student_performance/student-por.parquet\").to_pandas()\n",
    "orders_df = pd.read_parquet('data/tpch/orders.parquet')\n",
    "lineitem_df = pd.read_parquet('data/tpch/lineitem.parquet')\n",
    "\n",
    "# drop last row\n",
    "if USE_EVEN_NUMBER_OF_INPUT_ROWS:\n",
    "    if orders_df.shape[0] % 2 != 0:\n",
    "        print(f\"Input data has odd number of rows ({orders_df.shape[0]}), dropping last row to make it even.\")\n",
    "        orders_df = orders_df.iloc[:-1]\n",
    "orders_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df['o_orderkey'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(orders_df['o_orderdate'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineitem_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization (Pre-filter): Take out all the rows that will never be selected from the original dataframes.\n",
    "orders_df = orders_df[\n",
    "    (orders_df['o_orderpriority'] == '1-URGENT') &\n",
    "    (orders_df['o_orderdate'] >= date(1993, 7, 1)) &\n",
    "    (orders_df['o_orderdate'] < date(1993, 10, 1))\n",
    "]\n",
    "\n",
    "# 2. Filter lineitem rows where commit date is before receipt date.\n",
    "lineitem_df = lineitem_df[lineitem_df['l_commitdate'] < lineitem_df['l_receiptdate']]\n",
    "\n",
    "def runquery(orders_df: DataFrame, lineitem_df: DataFrame) -> int:\n",
    "    # orders_filtered = orders_df[\n",
    "    #     (orders_df['o_orderpriority'] == '1-URGENT') &\n",
    "    #     (orders_df['o_orderdate'] >= date(1993, 7, 1)) &\n",
    "    #     (orders_df['o_orderdate'] < date(1993, 10, 1))\n",
    "    # ]\n",
    "\n",
    "    # # 2. Filter lineitem rows where commit date is before receipt date.\n",
    "    # lineitem_filtered = lineitem_df[lineitem_df['l_commitdate'] < lineitem_df['l_receiptdate']]\n",
    "\n",
    "    # Optimization (Pre-filter): we have already filtered, no need to do so here\n",
    "    orders_filtered = orders_df\n",
    "    lineitem_filtered = lineitem_df\n",
    "\n",
    "    # 3. Identify orders that have at least one valid lineitem.\n",
    "    # Using .isin() to mimic the SQL EXISTS clause:\n",
    "    valid_orders = orders_filtered[\n",
    "        orders_filtered['o_orderkey'].isin(lineitem_filtered['l_orderkey'])\n",
    "    ]\n",
    "\n",
    "    # 4. Count the valid orders.\n",
    "    order_count = valid_orders.shape[0]\n",
    "    return order_count\n",
    "runquery(orders_df, lineitem_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Query Setup\n",
    "SAMPLES = 8192\n",
    "assert SAMPLES % 2 == 0, \"We need an even number of samples for paired sampling.\"\n",
    "number_of_pairs = SAMPLES // 2\n",
    "\n",
    "OUTPUT_COLS = ['count']\n",
    "INPUT_COLS = ['count']\n",
    "\n",
    "true_result = np.array([runquery(orders_df, lineitem_df)]) # Save the true result of the query for later\n",
    "#true_result = np.divide(true_result, 2) # manually correct count = count * 2\n",
    "\n",
    "number_of_contributing_rows = 999  # hardcode number of eligible rows\n",
    "\n",
    "def poisson_paired_sample(df: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "    \"\"\"\n",
    "    This will select a subset of indices, where each index is selected with probability 0.5.\n",
    "    The first result is the dataframe composed of the selected rows.\n",
    "    The second result is the complement / the dataframe composed of the rows that were not selected.\n",
    "    \"\"\"\n",
    "    mask = np.random.random_sample(len(df)) < 0.5  # Generates a bitmask of length df.shape[0] where each bit is 1 with probability 0.5\n",
    "    selected = df[mask]\n",
    "    not_selected = df[~mask]\n",
    "    return selected, not_selected\n",
    "\n",
    "def half_paired_sample(df: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "    \"\"\"\n",
    "    This will select half of the row indices from the dataframe at random.\n",
    "    The first result is the dataframe composed of the selected rows.\n",
    "    The second result is the complement / the dataframe composed of the rows that were not selected.\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(df.index, size=(df.shape[0] // 2), replace=False)\n",
    "    not_indices = list(set(df.index) - set(indices))\n",
    "    selected: DataFrame = df.loc[indices]\n",
    "    not_selected: DataFrame = df.loc[not_indices]\n",
    "    return (selected, not_selected)\n",
    "\n",
    "def sample_using_chosen_method(df: DataFrame) -> Tuple[DataFrame, DataFrame]:\n",
    "    if SAMPLING_METHOD == 'poisson':\n",
    "        return poisson_paired_sample(df)\n",
    "    elif SAMPLING_METHOD == 'half':\n",
    "        return half_paired_sample(df)\n",
    "\n",
    "def generate_samples(laplace_lambda: float = 1.0, alpha: int = 10) -> List[np.ndarray]:\n",
    "    # Apply thresholding: if there are less than alpha samples\n",
    "    # (with some Laplacian noise), then we don't subsample and return None.\n",
    "    laplace_noise: float = np.random.laplace(scale=laplace_lambda)\n",
    "\n",
    "    if number_of_contributing_rows + laplace_noise < alpha:  # if we don't get enough results from the query\n",
    "        print(\"There are not enough rows contributing to the result for PAC to be meaningful.\")\n",
    "        return []\n",
    "    \n",
    "    number_of_pairs = SAMPLES // 2\n",
    "    out_np: List[np.ndarray] = []\n",
    "    for i in range(number_of_pairs):\n",
    "        for temp_df in sample_using_chosen_method(orders_df.reset_index(drop=True)):  # reset index to sequential\n",
    "            out = runquery(temp_df, lineitem_df)\n",
    "            out *= 2  # manually correct count = count * 2\n",
    "            out_np.append(np.array([out]))\n",
    "\n",
    "    return out_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = generate_samples()\n",
    "#s\n",
    "\n",
    "#pd.DataFrame(s, columns=INPUT_COLS).hist(\"count\")\n",
    "#pd.DataFrame(s, columns=INPUT_COLS).hist(\"sum\")\n",
    "#pd.DataFrame(s, columns=INPUT_COLS).hist(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute variance of pd.DataFrame(s, columns=INPUT_COLS)['sum']\n",
    "#np.var(pd.DataFrame(s, columns=INPUT_COLS)['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute PAC Noise\n",
    "def get_pac_noise_scale(out_np_raw: List[np.ndarray],\n",
    "                           max_mi: float = 1./4) -> np.ndarray:\n",
    "    if out_np_raw is None or len(out_np_raw) == 0:\n",
    "        raise ValueError(\"Input list out_np cannot be empty.\")\n",
    "    out_np = out_np_raw.copy()\n",
    "    dimensions: int = len(out_np[0])\n",
    "\n",
    "    out_np_2darr = [np.atleast_2d(o) for o in out_np] # make sure all the DF -> np.ndarray conversions result in 2d arrays\n",
    "\n",
    "    est_y: np.ndarray = np.stack(out_np_2darr, axis=-1).reshape(dimensions, len(out_np))  # shape (dimensions, samples)\n",
    "    print(f\"est_y.shape: {est_y.shape}\")\n",
    "    print(f\"est_y: {est_y}\")\n",
    "\n",
    "    # get the scale in each basis direction\n",
    "    fin_var: np.ndarray = np.var(est_y, axis=1)  # shape (dimensions,)\n",
    "    print(f\"fin_var: {fin_var}\")\n",
    "    sqrt_total_var: np.floating[Any] = np.sum(np.sqrt(fin_var))\n",
    "    print(f\"sqrt_total_var: {sqrt_total_var}\")\n",
    "\n",
    "    pac_noise: np.ndarray = (1./(2*max_mi)) * sqrt_total_var * np.sqrt(fin_var)  # scale of the PAC noise\n",
    "    print(f\"For mi={max_mi}, we should add noise from a normal distribution with scale...\")\n",
    "    print(f\"\\t{1./(2*max_mi)} * {sqrt_total_var} * {np.sqrt(fin_var)} = {pac_noise}\")\n",
    "    print(f\"pac_noise: {pac_noise}\")\n",
    "    return pac_noise\n",
    "\n",
    "def do_pac_and_release(out_np: List[np.ndarray],\n",
    "                       max_mi: float = 1./4,\n",
    "                       scale: np.ndarray = None,\n",
    "                       chosen_index: int = None) -> np.ndarray:\n",
    "    \n",
    "    if scale is not None:\n",
    "        pac_noise: np.ndarray = scale\n",
    "    else:\n",
    "        pac_noise = get_pac_noise_scale(out_np, max_mi)\n",
    "\n",
    "    pac_noises_to_add: np.ndarray = np.random.normal(loc=0, scale=pac_noise)\n",
    "    # Add noise element-wise to the outputs\n",
    "    if chosen_index is None:  # allow for overriding the chosen index to specify which sample to use for the release\n",
    "        chosen_index = np.random.choice(range(SAMPLES))\n",
    "    pac_release = out_np[chosen_index] + pac_noises_to_add\n",
    "    \n",
    "    return pac_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_np = np.array([np.random.normal(loc=500, scale=10, size=8192),\n",
    "#                    np.random.normal(loc=100, scale=5, size=8192),\n",
    "#                    np.random.normal(loc=0, scale=200, size=8192)]).T.tolist()\n",
    "# print(\"We have generated some fake data.\")\n",
    "# print(f\"The standard deviation is {np.std(out_np, axis=0)}\")\n",
    "# print(f\"The variance of the data is {np.var(out_np, axis=0)}\")\n",
    "# print(f\"The mean of the data is {np.mean(out_np, axis=0)}\")\n",
    "# get_pac_noise_scale(out_np, 1./2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replicate the experiment that runs for epsilon=0.01, 0.1, 1, 10\n",
    "we use mi=1, 1/4, 1/16, 1/64, 1/256 in the hopes of encompassing a similar range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_OPTIONS = [1/64, 1/32, 1/16, 1/4, 1., 2., 4., 16.]\n",
    "EXPERIMENTS = 1000\n",
    "\n",
    "if GENERATE:\n",
    "    # df = pd.DataFrame([], columns=['mi', 'count', 'sum', 'mean', 'var'])\n",
    "    experiment_results = []\n",
    "    saved_steps = []\n",
    "\n",
    "    out_np = generate_samples() # generate samples (results of running the query)\n",
    "    print(f\"Generate samples... {len(out_np)} samples generated.\")\n",
    "\n",
    "    for mi in MI_OPTIONS:\n",
    "        scale = get_pac_noise_scale(out_np, mi) # estimate the stability of the query\n",
    "        print(f\"mi={mi}, scale={scale}\")\n",
    "        \n",
    "        for e in range(EXPERIMENTS):\n",
    "            # for each PAC release at this MI, we will choose a sample from the pre-generated out_np list and add noise to it\n",
    "            steps = {\n",
    "                \"mi\": mi,\n",
    "                \"scale\": scale,\n",
    "            }\n",
    "\n",
    "            # choose our sample\n",
    "            chosen_index = np.random.choice(range(SAMPLES))\n",
    "            chosen_sample = out_np[chosen_index].copy()\n",
    "            steps[\"chosen_sample\"] = chosen_sample\n",
    "            \n",
    "            # add noise to it\n",
    "            chosen_noise = np.random.normal(loc=0, scale=scale)\n",
    "            steps[\"chosen_noise\"] = chosen_noise\n",
    "\n",
    "            release = chosen_sample + chosen_noise # do_pac_and_release(out_np, mi, scale, chosen_index)\n",
    "\n",
    "            #print(f\"sample(#{chosen_index}):{chosen_sample} + noise:{chosen_noise} = {release}\")\n",
    "            steps[\"release\"] = release\n",
    "            #release[0] *= 2   # manually correct count = count * 2\n",
    "\n",
    "            # manually add sum as count * mean\n",
    "            #noisy_output = [noisy_output[0], noisy_output[0] * noisy_output[1], noisy_output[1]]\n",
    "            #chosen_sample = [chosen_sample[0], chosen_sample[0] * chosen_sample[1], chosen_sample[1]]\n",
    "            experiment_results.append([mi, *release])\n",
    "            saved_steps.append(steps)\n",
    "    \n",
    "    df = pd.DataFrame(experiment_results, columns=['mi', *OUTPUT_COLS])\n",
    "    \n",
    "    # Save the new data to outputs/...\n",
    "    df.to_parquet(f'{OUTPUT_DIR}/pac_results.parquet')\n",
    "    with open(f'{OUTPUT_DIR}/experiment_results.pkl', 'wb') as f:\n",
    "        pickle.dump(experiment_results, f)\n",
    "    with open(f'{OUTPUT_DIR}/saved_steps.pkl', 'wb') as f:\n",
    "        pickle.dump(saved_steps, f)\n",
    "else:\n",
    "    df = pq.read_table(f\"{OUTPUT_DIR}/pac_results.parquet\").to_pandas()\n",
    "\n",
    "    with open(f'{OUTPUT_DIR}/experiment_results.pkl', 'rb') as f:\n",
    "        experiment_results = pickle.load(f)\n",
    "    with open(f'{OUTPUT_DIR}/saved_steps.pkl', 'rb') as f:\n",
    "        saved_steps = pickle.load(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_steps_df_temp = pd.DataFrame(saved_steps)\n",
    "saved_steps_df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reconstruct Saved Steps\n",
    "# Get list of keys from the first dict, excluding 'mi' since it's not a list\n",
    "steps = [k for k in saved_steps[0].keys() if k not in ('mi')] # pull keys from saved_steps[0]\n",
    "\n",
    "saved_steps_df_temp = pd.DataFrame(saved_steps)\n",
    "\n",
    "# Create expanded columns using comprehension\n",
    "expanded = {\n",
    "    'mi': saved_steps_df_temp['mi'],\n",
    "    **{f'{step}_{col}': saved_steps_df_temp[step].str[i] \n",
    "        for step in steps\n",
    "        for i, col in enumerate(OUTPUT_COLS)}\n",
    "}\n",
    "\n",
    "# Create MultiIndex DataFrame using OUTPUT_COLS\n",
    "saved_steps_df = pd.DataFrame(expanded)\n",
    "saved_steps_df.columns = pd.MultiIndex.from_tuples([('mi',''), *[  # multiindex so that we can do things like saved_steps_df['release'][<aggregation>]\n",
    "    (step, col) for step in steps for col in OUTPUT_COLS\n",
    "]], names=[\"step\", \"query\"])\n",
    "saved_steps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_steps_df.groupby('mi').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_steps_df[saved_steps_df['mi'] == 1/4]['chosen_sample']['count'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('mean' in OUTPUT_COLS):\n",
    "    print(\"Mean of chosen_sample['mean'] for mi = 1/32\")\n",
    "    saved_steps_df[saved_steps_df['mi'] == 1/32]['chosen_sample']['mean'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('mean' in OUTPUT_COLS):\n",
    "    print(\"Mean of release['mean'] for mi = 1/32\")\n",
    "    saved_steps_df[saved_steps_df['mi'] == 1/32]['release']['mean'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG: Save the variables from computing the PAC noises for later reverse engineering\n",
    "#if GENERATE:\n",
    "#    saved_pac_variables_df = pd.DataFrame(saved_pac_variables, columns=['mi', 'out_np_0', 'fin_var', 'sqrt_total_var', 'pac_noise', 'pac_noises_to_add', 'pac_release'])\n",
    "#    saved_pac_variables_df.to_parquet(f'{OUTPUT_DIR}/saved_pac_variables.parquet')\n",
    "#else:\n",
    "#    saved_pac_variables_df = pq.read_table(f\"{OUTPUT_DIR}/saved_pac_variables.parquet\").to_pandas()\n",
    "#saved_pac_variables_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing PAC Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to calculate errors\n",
    "def absolute_scaled_error(est: np.ndarray, actual: np.ndarray) -> np.ndarray:\n",
    "    return np.abs(est - actual)\n",
    "def relative_error_percent(est: np.ndarray, actual: np.ndarray) -> np.ndarray:\n",
    "    return (np.abs(est - actual) / actual) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT_COLS = OUTPUT_COLS\n",
    "ERROR_COLS = [*[f'absolute error {i}' for i in OUTPUT_COLS], *[f'relative error {i}' for i in OUTPUT_COLS]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute errors for PAC (or import from saved file)\n",
    "if GENERATE:\n",
    "    errors_list = []\n",
    "    for i, row in df.iterrows():\n",
    "        mi = row['mi']\n",
    "        r = row[OUTPUT_COLS].to_numpy()\n",
    "        errors_list.append([mi, *absolute_scaled_error(r, true_result), *relative_error_percent(r, true_result)])\n",
    "    pac_errors_df = pd.DataFrame(errors_list, columns=['mi', *ERROR_COLS])\n",
    "    pac_errors_df.to_parquet(f\"{OUTPUT_DIR}/pac_errors.parquet\")  # overwrite saved\n",
    "else:\n",
    "    pac_errors_df = pq.read_table(f\"{OUTPUT_DIR}/pac_errors.parquet\").to_pandas()\n",
    "pac_errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GENERATE:\n",
    "#     mean_df = df.groupby('mi').mean()[['count', 'sum', 'mean', 'var']]\n",
    "#     std_df = df.groupby('mi').std()[['count', 'sum', 'mean', 'var']].abs()\n",
    "#     mean_df, std_df\n",
    "\n",
    "#     # save mean_df, std_df to parquet\n",
    "#     mean_df.to_parquet('{OUTPUT_DIR}/pac-mean_df_student-por.parquet')\n",
    "#     std_df.to_parquet('{OUTPUT_DIR}/pac-std_df_student-por.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing DP Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of PAC vs DP\n",
    "- import the DP data\n",
    "- create a dataframe (`all_df`) containing the query outputs\n",
    "    - in this df, each output is the {mean, variance} aggregation over the 500 experiments, grouped by the type of privacy mechanism used (pac or dp) and privacy factor (mi or epsilon)\n",
    "- create a dataframe in the same pattern containing the computed error for each privitized output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping# Import Chais saved data from running PipelineDP and use it to compare.\n",
    "dp_results_df = pq.read_table(f\"{OUTPUT_DIR}/dp-q4/dp_results.parquet\").to_pandas()\n",
    "\n",
    "# compute sum from count and mean\n",
    "dp_results_df['sum'] = dp_results_df['count'] * dp_results_df['mean']\n",
    "dp_results_df = dp_results_df[['mi', 'count', 'sum', 'mean']]\n",
    "\n",
    "# filter to just MI_OPTIONS\n",
    "dp_results_df = dp_results_df[dp_results_df['mi'].isin(MI_OPTIONS)]\n",
    "\n",
    "dp_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping# compute errors for DP (or import from saved file)\n",
    "GENERATE_DP = True\n",
    "if GENERATE_DP:\n",
    "    errors_list = []\n",
    "    for i, row in dp_results_df.iterrows():\n",
    "        mi = row['mi']\n",
    "        r = row[OUTPUT_COLS].to_numpy()\n",
    "        errors_list.append([mi, *absolute_scaled_error(r, true_result), *relative_error_percent(r, true_result)])\n",
    "    dp_errors_df = pd.DataFrame(errors_list, columns=['mi', *ERROR_COLS])\n",
    "    dp_errors_df.to_parquet(f\"{OUTPUT_DIR}/dp-q4/dp_errors.parquet\")  # overwrite saved\n",
    "else:\n",
    "    dp_errors_df = pq.read_table(f\"{OUTPUT_DIR}/dp-q4/dp_errors.parquet\").to_pandas()\n",
    "dp_errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make one merged DF to compare results\n",
    "all_df = pd.concat([  # we will call mi and epsilon the \"factor\" so that the schemas match\n",
    "    #dp_results_df.assign(type='dp').rename(columns={'eps': 'factor'}), # switch from eps to mi for indexing DP results\n",
    "    #dp_results_df.assign(type='dp').rename(columns={'mi': 'factor'}), # comment out DP results until we have some\n",
    "    df.assign(type='pac').rename(columns={'mi': 'factor'})\n",
    "])\n",
    "all_df = all_df.groupby(['type', 'factor']).agg(['mean', 'var'])\n",
    "all_df.columns = [' '.join(col).strip() for col in all_df.columns.values]  # Flatten the columns\n",
    "#all_df.to_parquet(f\"{OUTPUT_DIR}/all_results.parquet\")  # Save the merged results\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make another merged df for the errors\n",
    "errors_df = pd.concat([  # we will call mi and epsilon the \"factor\" so that the schemas match\n",
    "    #dp_errors_df.assign(type='dp').rename(columns={'eps': 'factor'}), # switch from eps to mi for indexing DP results\n",
    "    #dp_errors_df.assign(type='dp').rename(columns={'mi': 'factor'}), # comment out DP results until we have some\n",
    "    pac_errors_df.assign(type='pac').rename(columns={'mi': 'factor'})\n",
    "])\n",
    "errors_df = errors_df.groupby(['type', 'factor']).agg(['mean', 'var'])\n",
    "# drop absolute * mean and relative * var columns from level 1\n",
    "#errors_df = errors_df.drop(columns=[('absolute error count', 'mean'), ('absolute error mean', 'mean'), ('relative error count', 'var'), ('relative error mean', 'var')])\n",
    "errors_df.columns = [' '.join(col).strip() for col in errors_df.columns.values]  # flatten the columns\n",
    "\n",
    "#errors_df.to_parquet(f\"{OUTPUT_DIR}/all_errors.parquet\")\n",
    "errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abs_error = errors_df.groupby('mi').mean()[['absolute error count', 'absolute error sum', 'absolute error mean', 'absolute error var']]\n",
    "#rel_error = errors_df.groupby('mi').mean()[['relative error count', 'relative error sum', 'relative error mean', 'relative error var']]\n",
    "\n",
    "#std_abs_error = errors_df.groupby('mi').std()[['absolute error count', 'absolute error sum', 'absolute error mean', 'absolute error var']]\n",
    "#sample_mean_rel_error = errors_df.groupby('mi').mean()[['relative error count', 'relative error sum', 'relative error mean', 'relative error var']]\n",
    "\n",
    "# save std_abs_error, sample_mean_rel_error to parquet\n",
    "# std_abs_error.to_parquet('outputs/pac-std_abs_error_student-por.parquet')\n",
    "# sample_mean_rel_error.to_parquet('outputs/pac-sample_mean_rel_error_student-por.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results\n",
    "\n",
    "We are reproducing Table 14 from this paper: http://arxiv.org/abs/2109.10789\n",
    "\n",
    "> TABLE 14: Experiments of the queries count, sum, mean, and var on the attribute Absences of the Portuguese education dataset containing 649 individuals (500 experiments per ε)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports for matplotlib\n",
    "import matplotlib as mpl\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#sns.reset_orig()\n",
    "\n",
    "# Michael's matplotlib defaults\n",
    "# set font to Times New Roman\n",
    "LATEX = False\n",
    "if LATEX:\n",
    "    mpl.rcParams['text.usetex'] = True\n",
    "    mpl.rcParams[\"font.family\"] = \"serif\"\n",
    "    mpl.rcParams[\"font.serif\"] = \"Times\"\n",
    "else:\n",
    "    mpl.rcParams['text.usetex'] = False\n",
    "    mpl.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    mpl.rcParams[\"mathtext.fontset\"] = \"stix\"\n",
    "    \n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "import matplotlib_inline.backend_inline\n",
    "#matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "mpl.rcParams['axes.titleweight'] = 'bold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "PAC_COLOR = 'tab:blue'\n",
    "DP_COLOR = 'tab:red'\n",
    "\n",
    "# Function to lighten a color by blending with white\n",
    "def lighten_color(color, amount=0.3):\n",
    "    rgba = mcolors.to_rgba(color)\n",
    "    return tuple(np.clip(np.array(rgba[:3]) + amount, 0, 1)) + (rgba[3],)\n",
    "\n",
    "# Create lighter colors\n",
    "PAC_LIGHT = lighten_color(PAC_COLOR, 0.3)\n",
    "DP_LIGHT = lighten_color(DP_COLOR, 0.3)\n",
    "\n",
    "# create legend, use color squares\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "pac_dp_legend_handles = [\n",
    "    Patch(facecolor=PAC_COLOR, edgecolor=PAC_COLOR, label='PAC'),\n",
    "    Patch(facecolor=DP_COLOR, edgecolor=DP_COLOR, label='DP')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduction of Table 14 (PAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reproduction of Table 14 (PAC)\n",
    "# Plotting each query type with their error metrics\n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 6))\n",
    "queries = OUTPUT_COLS\n",
    "#MI_OPTIONS = [1/64, 1/32, 1/16, 1/4, 1., 2., 4., 16.]\n",
    "\n",
    "# Loop through each query to create the subplots\n",
    "for i, query in enumerate(queries):\n",
    "    # go from top left, to top right, to bottom left, to bottom right\n",
    "    row = (i*2)//4\n",
    "    col = (i*2) % 4\n",
    "    ax1 = axs[row][col]\n",
    "    ax2 = axs[row][col+1]\n",
    "\n",
    "    # Plotting Sample Std of Absolute Scaled Error\n",
    "    TYPE = 'pac'\n",
    "    d = errors_df.loc[TYPE].loc[:, f'absolute error {query} var']\n",
    "    ax1.plot(d.index, d, label=query, marker='.', color=PAC_COLOR)\n",
    "             #label=query, marker='.')\n",
    "\n",
    "    ax1.set_yscale('log')\n",
    "\n",
    "    ax1.set_xscale('log', base=2)\n",
    "    xticks = MI_OPTIONS\n",
    "    xlabels = [f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in xticks]\n",
    "    ax1.set_xticks(xticks)\n",
    "    ax1.set_xticklabels(xlabels)\n",
    "\n",
    "    ax1.set_title(f'{query.capitalize()} Query')\n",
    "    ax1.set_xlabel('MI')\n",
    "    ax1.set_ylabel('Sample Std of the Absolute Scaled Error')\n",
    "\n",
    "    # Plotting Sample Mean of the Relative Error (%)\n",
    "    d = errors_df.loc[TYPE].loc[:, f'relative error {query} mean']\n",
    "    ax2.plot(d.index, d, label=query, marker='.', color=PAC_COLOR)\n",
    "\n",
    "    ax2.set_xscale('log', base=2)\n",
    "    xticks = MI_OPTIONS\n",
    "    xlabels = [f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in xticks]\n",
    "    ax2.set_xticks(xticks)\n",
    "    ax2.set_xticklabels(xlabels)\n",
    "\n",
    "    ax2.set_title(f'{query.capitalize()} Query')\n",
    "    ax2.set_xlabel('MI')\n",
    "    ax2.set_ylabel('Sample Mean of the Relative Error (%)')\n",
    "\n",
    "# add label in top left corner \"sum = count * mean\"\n",
    "#fig.text(0.75, 0.95, '(sum = count * mean)', ha='center', va='center')\n",
    "fig.text(0.05, 0.97, f'({number_of_contributing_rows} rows)', ha='center', va='center')\n",
    "fig.suptitle(\"Reproduction of Table 14 (PAC)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/table14_pac.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduction of Table 14 (DP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping### Reproduction of Table 14 (DP)\n",
    "# Plotting each query type with their error metrics\n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 6))\n",
    "queries = OUTPUT_COLS\n",
    "#MI_OPTIONS = [1/256, 1/128, 1/64, 1/16, 1/4, 1., 2., 4., 16.]\n",
    "\n",
    "# Loop through each query to create the subplots\n",
    "for i, query in enumerate(queries):\n",
    "    # go from top left, to top right, to bottom left, to bottom right\n",
    "    row = (i*2)//4\n",
    "    col = (i*2) % 4\n",
    "    ax1 = axs[row][col]\n",
    "    ax2 = axs[row][col+1]\n",
    "\n",
    "    # Plotting Sample Std of Absolute Scaled Error\n",
    "    TYPE = 'dp'\n",
    "    d = errors_df.loc[TYPE].loc[:, f'absolute error {query} var']\n",
    "    ax1.plot(d.index, d, label=query, marker='.', color=DP_COLOR)\n",
    "             #label=query, marker='.')\n",
    "    \n",
    "    ax1.set_yscale('log')\n",
    "\n",
    "    ax1.set_xscale('log', base=2)\n",
    "    xticks = MI_OPTIONS\n",
    "    xlabels = [f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in xticks]\n",
    "    ax1.set_xticks(xticks)\n",
    "    ax1.set_xticklabels(xlabels)\n",
    "\n",
    "    ax1.set_title(f'{query.capitalize()} Query')\n",
    "    ax1.set_xlabel('MI')\n",
    "    ax1.set_ylabel('Sample Std of the Absolute Scaled Error')\n",
    "\n",
    "    # Plotting Sample Mean of the Relative Error (%)\n",
    "    d = errors_df.loc[TYPE].loc[:, f'relative error {query} mean']\n",
    "    ax2.plot(d.index, d, label=query, marker='.', color=DP_COLOR)\n",
    "\n",
    "    ax2.set_xscale('log', base=2)\n",
    "    xticks = MI_OPTIONS\n",
    "    xlabels = [f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in xticks]\n",
    "    ax2.set_xticks(xticks)\n",
    "    ax2.set_xticklabels(xlabels)\n",
    "\n",
    "    ax2.set_title(f'{query.capitalize()} Query')\n",
    "    ax2.set_xlabel('MI')\n",
    "    ax2.set_ylabel('Sample Mean of the Relative Error (%)')\n",
    "\n",
    "fig.suptitle(\"Reproduction of Table 14 (DP)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/table14_dp.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both on the same axes, aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "### Both on the same axes, aligned\n",
    "# Plotting each query type with their error metrics\n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 6))\n",
    "queries = OUTPUT_COLS\n",
    "#MI_OPTIONS = [1/256, 1/128, 1/64, 1/32, 1/16, 1/4, 1., 2., 4., 16.]\n",
    "\n",
    "# Loop through each query to create the subplots\n",
    "for i, query in enumerate(queries):\n",
    "    # go from top left, to top right, to bottom left, to bottom right\n",
    "    row = (i*2)//4\n",
    "    col = (i*2) % 4\n",
    "    ax1 = axs[row][col]\n",
    "    ax2 = axs[row][col+1]\n",
    "\n",
    "    # Plotting Sample Std of Absolute Scaled Error\n",
    "    d = errors_df.loc['pac'].loc[:, f'absolute error {query} var']\n",
    "    ax1.plot(d.index, d, label=query, marker='.', color=PAC_COLOR)\n",
    "\n",
    "    d = errors_df.loc['dp'].loc[:, f'absolute error {query} var']\n",
    "    #print(d)\n",
    "    ax1.plot(d.index, d, label=query, marker='.', color=DP_COLOR)\n",
    "\n",
    "    ax1.set_xscale('log', base=2)\n",
    "    xticks = MI_OPTIONS\n",
    "    xlabels = [f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in xticks]\n",
    "    ax1.set_xticks(xticks)\n",
    "    ax1.set_xticklabels(xlabels)\n",
    "\n",
    "    ax1.set_title(f'{query.capitalize()} Query')\n",
    "    ax1.set_ylabel('Sample Std of the Absolute Scaled Error')\n",
    "    ax1.set_xlabel('MI')\n",
    "\n",
    "    # Plotting Sample Mean of the Relative Error (%)\n",
    "    d = errors_df.loc['pac'].loc[:, f'relative error {query} mean']\n",
    "    ax2.plot(d.index, d, label=query, marker='.', color=PAC_COLOR)\n",
    "\n",
    "    d = errors_df.loc['dp'].loc[:, f'relative error {query} mean']\n",
    "    ax2.plot(d.index, d, label=query, marker='.', color=DP_COLOR)\n",
    "    \n",
    "    ax2.set_xscale('log', base=2)\n",
    "    xticks = MI_OPTIONS\n",
    "    xlabels = [f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in xticks]\n",
    "    ax2.set_xticks(xticks)\n",
    "    ax2.set_xticklabels(xlabels)\n",
    "\n",
    "    ax2.set_title(f'{query.capitalize()} Query')\n",
    "    ax2.set_ylabel('Sample Mean of the Relative Error (%)')\n",
    "    ax2.set_xlabel('MI')\n",
    "\n",
    "# add label in top left corner \"sum = count * mean\"\n",
    "#fig.text(0.75, 0.94, '(sum = count * mean)', ha='center', va='center')\n",
    "\n",
    "fig.legend(handles=pac_dp_legend_handles, loc='upper right', frameon=False)\n",
    "\n",
    "fig.text(0.05, 0.97, f'({number_of_contributing_rows} rows)', ha='center', va='center')\n",
    "fig.suptitle(\"Reproduction of Table 14 (PAC vs DP)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/table14_pac_vs_dp.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additonal Extra Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the actual experiment outputs used (PAC)\n",
    "# Plotting each query type with their error metrics\n",
    "fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n",
    "queries = OUTPUT_COLS\n",
    "#MI_OPTIONS = [1/64, 1/32, 1/16, 1/4, 1., 2., 4., 16.]\n",
    "\n",
    "# Loop through each query to create the subplots\n",
    "for i, query in enumerate(queries):\n",
    "    # go from top left, to top right, to bottom left, to bottom right\n",
    "    row = i//2\n",
    "    col = i%2\n",
    "    ax = axs[row][col]\n",
    "\n",
    "    TYPE = 'pac'\n",
    "    ax.scatter(df['mi'], df[query], label=query, marker='.', alpha=0.5, color=PAC_COLOR)\n",
    "    \n",
    "    ax.axhline(y=true_result[i], color='black', linestyle='--', label='True Result')\n",
    "\n",
    "    #for mi in MI_OPTIONS:\n",
    "        #d = df.iloc[:, [0, i]].where(df['mi'] == mi).dropna()\n",
    "        #ax.scatter(d.iloc[:, 0], d.iloc[:, 1], label=query, marker='.', alpha=0.5, color=PAC_COLOR)\n",
    "        #ax.boxplot(d.iloc[:, 1], positions=[mi], widths=0.1, showfliers=False)\n",
    "    \n",
    "    ax.set_xscale('log', base=2)\n",
    "    xticks = MI_OPTIONS\n",
    "    xlabels = [f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in xticks]\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xlabels)\n",
    "\n",
    "    ax.set_title(f'{query.capitalize()} Query')\n",
    "    ax.set_xlabel('MI')\n",
    "    ax.set_ylabel('Result')\n",
    "\n",
    "fig.legend(handles=[\n",
    "    Patch(facecolor=PAC_COLOR, edgecolor=PAC_COLOR, label='PAC'),\n",
    "    Line2D([0], [0], color='black', linestyle='--', label='True Result')\n",
    "], loc='upper right', frameon=False)\n",
    "\n",
    "# add label in top left corner \"sum = count * mean\"\n",
    "#fig.text(0.75, 0.94, '(sum = count * mean)', ha='center', va='center')\n",
    "fig.text(0.05, 0.97, f'({number_of_contributing_rows} rows)', ha='center', va='center')\n",
    "fig.suptitle(\"Actual outputs of query on subsamples (PAC)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/actual_outputs_pac.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping### Plotting the actual experiment outputs used (DP)\n",
    "# Plotting each query type with their error metrics\n",
    "fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n",
    "queries = OUTPUT_COLS\n",
    "#MI_OPTIONS = [1/256, 1/128, 1/64, 1/16, 1/4, 1., 2., 4., 16.]\n",
    "\n",
    "# Loop through each query to create the subplots\n",
    "for i, query in enumerate(queries):\n",
    "    # go from top left, to top right, to bottom left, to bottom right\n",
    "    row = i//2\n",
    "    col = i%2\n",
    "    ax = axs[row][col]\n",
    "\n",
    "    TYPE = 'dp'\n",
    "\n",
    "    ax.scatter(dp_results_df['mi'], dp_results_df[query], label=query, marker='.', alpha=0.5, color=DP_COLOR)\n",
    "    ax.axhline(y=true_result[i], color='black', linestyle='--', label='True Result')\n",
    "\n",
    "    ax.set_xscale('log', base=2)\n",
    "    xticks = MI_OPTIONS\n",
    "    xlabels = [f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in xticks]\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xlabels)\n",
    "\n",
    "    ax.set_title(f'{query.capitalize()} Query')\n",
    "    ax.set_xlabel('MI')\n",
    "    ax.set_ylabel('Result')\n",
    "\n",
    "fig.legend(handles=[\n",
    "    Patch(facecolor=DP_COLOR, edgecolor=DP_COLOR, label='DP'),\n",
    "    Line2D([0], [0], color='black', linestyle='--', label='True Result')\n",
    "], loc='upper right', frameon=False)\n",
    "\n",
    "# add label in top left corner \"sum = count * mean\"\n",
    "#fig.text(0.75, 0.94, '(sum = count * mean)', ha='center', va='center')\n",
    "\n",
    "fig.suptitle(\"Actual outputs of query trials (DP)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/actual_outputs_dp.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping### Both, but with side by side fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n",
    "fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n",
    "queries = OUTPUT_COLS\n",
    "#MI_OPTIONS = [1/256, 1/128, 1/64, 1/32, 1/16, 1/4, 1., 2., 4., 16.]\n",
    "\n",
    "# Loop through each query to create the subplots\n",
    "for i, query in enumerate(queries):\n",
    "    # go from top left, to top right, to bottom left, to bottom right\n",
    "    row = i//2\n",
    "    col = i%2\n",
    "    ax = axs[row][col]\n",
    "\n",
    "    ax.linewidth = 0.25\n",
    "\n",
    "    # correct answer\n",
    "    ax.axhline(y=true_result[i], color='gray', linestyle='--', label='True Result')\n",
    "\n",
    "    for mi in MI_OPTIONS: # categorical variable\n",
    "        position = MI_OPTIONS.index(mi) * 2\n",
    "\n",
    "        TYPE = 'pac'\n",
    "\n",
    "        pac_style = {\n",
    "            'boxprops': dict(facecolor=PAC_LIGHT),\n",
    "            'medianprops': dict(color='black')\n",
    "        }\n",
    "        \n",
    "        # mi = mi and query is correct\n",
    "        d = df[df['mi'] == mi][query].values\n",
    "        ax.boxplot(d, positions=[position], widths=0.6, patch_artist=True, showfliers=False, **pac_style)\n",
    "\n",
    "        position += 1\n",
    "        TYPE = 'dp'\n",
    "\n",
    "        dp_style = {\n",
    "            'boxprops': dict(facecolor=DP_LIGHT),\n",
    "            'medianprops': dict(color='black')\n",
    "        }\n",
    "\n",
    "        # mi = mi and query is correct\n",
    "        d = dp_results_df[dp_results_df['mi'] == mi][query].values\n",
    "        ax.boxplot(d, positions=[position], widths=0.6, patch_artist=True, showfliers=False, **dp_style)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # override xticks, one for each pair of boxplots\n",
    "    # the x axis is actually artificial here to plot both PAC and DP side by side\n",
    "    xticks = [0.5+2*i for i in range(len(MI_OPTIONS))]\n",
    "    ax.set_xticks(xticks)\n",
    "\n",
    "    xlabels = [f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in MI_OPTIONS]\n",
    "    ax.set_xticklabels(xlabels)\n",
    "\n",
    "    ax.set_title(f'{query.capitalize()} Query')\n",
    "    ax.set_xlabel('MI')\n",
    "    ax.set_ylabel('Result')\n",
    "\n",
    "fig.legend(handles=[\n",
    "    Patch(facecolor=PAC_COLOR, edgecolor=PAC_COLOR, label='PAC'),\n",
    "    Patch(facecolor=DP_COLOR, edgecolor=DP_COLOR, label='DP'),\n",
    "    Line2D([0], [0], color='gray', linestyle='--', label='True Result')\n",
    "], loc='upper right', frameon=False)\n",
    "\n",
    "# add label in top left corner \"sum = count * mean\"\n",
    "#fig.text(0.75, 0.94, '(sum = count * mean)', ha='center', va='center')\n",
    "fig.text(0.05, 0.97, f'({number_of_contributing_rows} rows)', ha='center', va='center')\n",
    "fig.suptitle(\"Actual outputs of query trials (PAC vs DP)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/actual_outputs_dp_vs_pac.jpg\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('mi').var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables\n",
    "from plottable import ColumnDefinition, Table\n",
    "from plottable.cmap import normed_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scale of noise added to [Count, Mean] (scale param passed to Normal noise function)\")\n",
    "saved_steps_df[['mi','scale']].groupby('mi').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of how much noise is added to the data, by looking at the variance of the distributions from which noise is sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping # Wait until we have DP data to compare\n",
    "dp_var_noise = {  # from pipelinedp repo, dp_basic_mean_variances.csv\n",
    "    #0.00390625: 0.2784089313555703,\n",
    "    #0.0078125: 0.19648087910574175,\n",
    "    0.015625: 0.13837628104247293,\n",
    "    0.0625: 0.06750340282072995,\n",
    "    0.25: 0.03001721300521553,\n",
    "    1.0: 0.004282722801885366,\n",
    "    2.0: 0.004282722801885366,\n",
    "    4.0: 0.004282722801885366,\n",
    "    16.0: 0.004282722801885366\n",
    "}\n",
    "d = dp_var_noise\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "plt.plot(d.keys(), d.values(), marker='.', label='DP', color=DP_COLOR)\n",
    "plt.xlabel('MI')\n",
    "ax.set_xticks(list(d.keys()))\n",
    "ax.set_xticklabels([f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in d.keys()])\n",
    "plt.suptitle('Noise added by DP to Mean query (scale param passed to Laplace noise function)')\n",
    "plt.title('DP')\n",
    "plt.savefig(f\"{OUTPUT_DIR}/noise_distribution_scale_meanquery_dp.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert not any(any(saved_steps_df[['mi','scale']].groupby('mi').var().values)) # assume we are only computing scale once per MI\n",
    "if ('mean' in OUTPUT_COLS):\n",
    "    pac_var_noise = dict(zip(MI_OPTIONS, saved_steps_df[['mi','scale']].groupby('mi').mean()[[('scale','mean')]].to_numpy().flatten()))\n",
    "    pac_var_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the same for PAC: what is the scale of the distribution from which noise is sampled to add to PAC?\n",
    "if ('mean' in OUTPUT_COLS): \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    d = pac_var_noise\n",
    "    plt.plot(d.keys(), d.values(), marker='.', label='PAC', color=PAC_COLOR)\n",
    "    plt.xlabel('MI')\n",
    "    ax.set_xticks(list(d.keys()))\n",
    "    ax.set_xticklabels([f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in d.keys()])\n",
    "    plt.suptitle('Noise added by PAC to Mean query (scale param passed to Normal noise function)')\n",
    "    plt.title('PAC')\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/noise_distribution_scale_meanquery_pac.jpg\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much of PAC error is due to sampling? Compute % error on our samples before noise was added\n",
    "sample_errors = [relative_error_percent(s, true_result) for s in saved_steps_df['chosen_sample'].to_numpy()]\n",
    "sample_errors_df = pd.DataFrame(sample_errors, columns=OUTPUT_COLS)\n",
    "sample_errors_df.insert(0, 'mi', saved_steps_df['mi'].values)  # Insert 'mi' at the beginning\n",
    "print(\"PAC: Error from Subsampling (as % of true_result, without any noise being added)\")\n",
    "#print(\"First output: overall mean / Second output: grouped by MI\")\n",
    "display(sample_errors_df[OUTPUT_COLS].mean())\n",
    "display(sample_errors_df.groupby('mi').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=sample_errors_df.groupby('mi').mean()\n",
    "d.loc['all mi'] = sample_errors_df[OUTPUT_COLS].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2+(0.5*len(d.columns)), 3.5))\n",
    "\n",
    "col_defs = [\n",
    "    ColumnDefinition(\n",
    "        name=d.index.name,\n",
    "        #textprops={\"ha\": \"right\"},\n",
    "        width=0.25,\n",
    "        formatter=lambda x: (f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}') if type(x) != str else x,\n",
    "    ),\n",
    "    *[ColumnDefinition(\n",
    "        name=c,\n",
    "        textprops={\"ha\": \"right\"},\n",
    "        width=0.5,\n",
    "        formatter=lambda x: f\"{x:.3f}%\",\n",
    "    ) for c in d.columns]]\n",
    "tbl = Table(d,\n",
    "    column_definitions=col_defs,\n",
    "    row_dividers=True,\n",
    "    footer_divider=True,\n",
    "    ax=ax,\n",
    "    #textprops={\"fontsize\": 14},\n",
    "    #row_divider_kw={\"linewidth\": 1, \"linestyle\": (0, (1, 5))},\n",
    "    #col_label_divider_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    "    #column_border_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    "    )\n",
    "plt.title(\"PAC: Error from Subsampling (as % of true_result,\\n without any noise being added)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/tbl-pac_sample_error.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PAC: How much noise are we adding? (actual noise amounts sampled from the normal distribution, absolute value)\")\n",
    "# abs before mean\n",
    "saved_steps_df[['mi', 'chosen_noise']].apply(lambda x: np.abs(x)).groupby('mi').mean().rename(columns={'chosen_noise': 'abs(chosen_noise)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = saved_steps_df[['mi', 'chosen_noise']].apply(lambda x: np.abs(x)).groupby('mi').mean().rename(columns={'chosen_noise': 'abs(chosen_noise)'})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2+(0.5*len(d.columns)), 3.5))\n",
    "\n",
    "col_defs = [\n",
    "    ColumnDefinition(\n",
    "        name=d.index.name,\n",
    "        #textprops={\"ha\": \"right\"},\n",
    "        width=0.25,\n",
    "        formatter=lambda x: (f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}') if type(x) != str else x,\n",
    "    ),\n",
    "    *[ColumnDefinition(\n",
    "        name=c,\n",
    "        title='\\n'.join(c),\n",
    "        textprops={\"ha\": \"right\"},\n",
    "        width=0.5,\n",
    "        formatter=lambda x: f\"{x:.3f}\",\n",
    "    ) for c in d.columns]]\n",
    "tbl = Table(d,\n",
    "    column_definitions=col_defs,\n",
    "    row_dividers=True,\n",
    "    footer_divider=True,\n",
    "    ax=ax,\n",
    "    #textprops={\"fontsize\": 14},\n",
    "    #row_divider_kw={\"linewidth\": 1, \"linestyle\": (0, (1, 5))},\n",
    "    #col_label_divider_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    "    #column_border_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    "    )\n",
    "plt.title(\"PAC: How much noise are we adding? (actual noise\\n amounts sampled from the normal distribution,\\n absolute value)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/tbl-pac_chosen_noise.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avg. value of unnoised sample used by PAC\")\n",
    "display(saved_steps_df[['mi', 'chosen_sample']].groupby('mi').mean())  # average per MI, shouldn't be different\n",
    "display(saved_steps_df[['chosen_sample']].mean().to_frame(name='all mi').T)  # sample shouldn't change with MI, so we can find the overall average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = saved_steps_df[['mi', 'chosen_sample']].groupby('mi').mean()\n",
    "d.loc['all mi'] = saved_steps_df[['chosen_sample']].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2+(0.5*len(d.columns)), 3.5))\n",
    "\n",
    "col_defs = [\n",
    "    ColumnDefinition(\n",
    "        name=d.index.name,\n",
    "        #textprops={\"ha\": \"right\"},\n",
    "        width=0.25,\n",
    "        formatter=lambda x: (f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}') if type(x) != str else x,\n",
    "    ),\n",
    "    *[ColumnDefinition(\n",
    "        name=c,\n",
    "        title='\\n'.join(c),\n",
    "        textprops={\"ha\": \"right\"},\n",
    "        width=0.5,\n",
    "        formatter=lambda x: f\"{x:.3f}\",\n",
    "    ) for c in d.columns]]\n",
    "tbl = Table(d,\n",
    "    column_definitions=col_defs,\n",
    "    row_dividers=True,\n",
    "    footer_divider=True,\n",
    "    ax=ax,\n",
    "    #textprops={\"fontsize\": 14},\n",
    "    #row_divider_kw={\"linewidth\": 1, \"linestyle\": (0, (1, 5))},\n",
    "    #col_label_divider_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    "    #column_border_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    "    )\n",
    "plt.title(\"Avg. value of unnoised sample used by PAC\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/tbl-pac_unnoised_samples.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much total error is there in PAC? Compute % error on our samples after noise was added\n",
    "total_errors = [relative_error_percent(s, true_result) for s in saved_steps_df['release'].to_numpy()]\n",
    "total_errors_df = pd.DataFrame(total_errors, columns=OUTPUT_COLS)\n",
    "total_errors_df.insert(0, 'mi', saved_steps_df['mi'].values)  # Insert 'mi' at the beginning\n",
    "print(\"PAC: Total Error (as % of true_result)\")\n",
    "total_errors_df.groupby('mi').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=total_errors_df.groupby('mi').mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2+(0.5*len(d.columns)), 3.5))\n",
    "\n",
    "col_defs = [\n",
    "    ColumnDefinition(\n",
    "        name=d.index.name,\n",
    "        #textprops={\"ha\": \"right\"},\n",
    "        width=0.25,\n",
    "        formatter=lambda x: f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}',\n",
    "    ),\n",
    "    *[ColumnDefinition(\n",
    "        name=c,\n",
    "        textprops={\"ha\": \"right\"},\n",
    "        width=0.5,\n",
    "        formatter=lambda x: f\"{x:.3f}%\",\n",
    "    ) for c in d.columns]]\n",
    "tbl = Table(d,\n",
    "    column_definitions=col_defs,\n",
    "    row_dividers=True,\n",
    "    footer_divider=True,\n",
    "    ax=ax,\n",
    "    #textprops={\"fontsize\": 14},\n",
    "    #row_divider_kw={\"linewidth\": 1, \"linestyle\": (0, (1, 5))},\n",
    "    #col_label_divider_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    "    #column_border_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    "    )\n",
    "plt.title(\"PAC: Total Error (as % of true_result)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/tbl-pac_error_total.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PAC: Increase in Avg. % error due to noise being added (as % of true_result)\")\n",
    "delta_error_from_noise = pd.concat(\n",
    "    [\n",
    "        saved_steps_df['mi'].reset_index(drop=True),\n",
    "        pd.DataFrame(\n",
    "            (np.array([relative_error_percent(s, true_result) for s in saved_steps_df['release'].to_numpy()]) -\n",
    "             np.array([relative_error_percent(s, true_result) for s in saved_steps_df['chosen_sample'].to_numpy()])),\n",
    "            columns=OUTPUT_COLS\n",
    "        )\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "delta_error_from_noise.groupby('mi').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=delta_error_from_noise.groupby('mi').mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2+(0.5*len(d.columns)), 3.5))\n",
    "\n",
    "col_defs = [\n",
    "    ColumnDefinition(\n",
    "        name=d.index.name,\n",
    "        #textprops={\"ha\": \"right\"},\n",
    "        width=0.25,\n",
    "        formatter=lambda x: f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}',\n",
    "    ),\n",
    "    *[ColumnDefinition(\n",
    "        name=c,\n",
    "        textprops={\"ha\": \"right\"},\n",
    "        width=0.5,\n",
    "        formatter=lambda x: f\"{x:.3f}%\",\n",
    "    ) for c in d.columns]]\n",
    "tbl = Table(d,\n",
    "        column_definitions=col_defs,\n",
    "        row_dividers=True,\n",
    "        footer_divider=True,\n",
    "        ax=ax,\n",
    "        )\n",
    "plt.title(\"PAC: Increase in Avg. % error due to noise\\nbeing added (as % of true_result)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/tbl-pac_error_noise.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combined % Error Table\n",
    "d = pd.concat([\n",
    "    sample_errors_df.groupby('mi').mean(),  # % error from subsampling\n",
    "    delta_error_from_noise.groupby('mi').mean(),  # % error from noise\n",
    "    total_errors_df.groupby('mi').mean(),  # % total error\n",
    "], axis=1)\n",
    "d.columns = ['Subsampling', 'Added Noise', 'Total Error']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2+(0.5*len(d.columns)), 3.5))\n",
    "\n",
    "col_defs = [\n",
    "    ColumnDefinition(\n",
    "        name=d.index.name,\n",
    "        #textprops={\"ha\": \"right\"},\n",
    "        width=0.25,\n",
    "        formatter=lambda x: f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}',\n",
    "    ),\n",
    "    *[ColumnDefinition(\n",
    "        name=c,\n",
    "        group=\"Error Sources\",\n",
    "        textprops={\"ha\": \"right\"},\n",
    "        width=0.5,\n",
    "        formatter=lambda x: f\"{x:.3f}%\",\n",
    "    ) for c in d.columns[:-1]],\n",
    "    ColumnDefinition(\n",
    "        name=d.columns[-1],\n",
    "        textprops={\"ha\": \"right\"},\n",
    "        width=0.5,\n",
    "        formatter=lambda x: f\"{x:.3f}%\",\n",
    "    )]\n",
    "tbl = Table(d,\n",
    "            column_definitions=col_defs,\n",
    "            row_dividers=True,\n",
    "            footer_divider=True,\n",
    "            ax=ax,\n",
    "        )\n",
    "#plt.title(\"PAC: Error Breakdown\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/tbl-pac_error_breakdown.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n",
    "\n",
    "psample_errors = sample_errors_df.groupby('mi').mean()\n",
    "pnoise_errors = delta_error_from_noise.groupby('mi').mean()\n",
    "\n",
    "for i, query in enumerate(OUTPUT_COLS):\n",
    "    row = i//2\n",
    "    col = i%2\n",
    "    ax = axs[row][col]\n",
    "\n",
    "    ax.stackplot(psample_errors.index, psample_errors[query], pnoise_errors[query], \n",
    "        labels=['Sample Error', 'Noise Error'], colors=['orange', 'green'], alpha=0.6)\n",
    "    \n",
    "    ax.set_xscale('log', base=2)\n",
    "    xticks = MI_OPTIONS\n",
    "    xlabels = [f'$\\\\frac{{1}}{{{int(1/x)}}}$' if x < 1 else f'{int(x)}' for x in xticks]\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xlabels)\n",
    "\n",
    "\n",
    "    ax.set_title(f'{query.capitalize()} Query')\n",
    "    ax.set_xlabel('MI')\n",
    "    ax.set_ylabel('Error (%)')\n",
    "\n",
    "fig.legend(handles=[\n",
    "    Patch(facecolor='orange', edgecolor='orange', label='Noise from Sampling'),\n",
    "    Patch(facecolor='green', edgecolor='green', label='Added Noise'),\n",
    "], loc='upper right', frameon=False)\n",
    "\n",
    "# add label in top left corner \"sum = count * mean\"\n",
    "fig.text(0.05, 0.97, f'({number_of_contributing_rows} rows)', ha='center', va='center')\n",
    "fig.suptitle(\"PAC: Stacked Area Plot of Sources of Error\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(f\"{OUTPUT_DIR}/pac_error_breakdown.jpg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
